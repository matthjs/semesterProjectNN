{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Simple) RNN in Jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 872,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as np\n",
    "import math\n",
    "import numpy as onp\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from jax import grad, vmap, jit\n",
    "import jax\n",
    "import jax.numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 873,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre - and Postprocessing\n",
    "$$\n",
    "\\mathcal{D} : \\text{CSV} \\rightarrow [0,1]^{K \\times n_i}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 874,
   "metadata": {},
   "outputs": [],
   "source": [
    "def D(df):\n",
    "    time = df[\"time\"].values\n",
    "    notes = df[\"notes\"].values\n",
    "    velocity = df[\"velocity\"].values\n",
    "    h = {}\n",
    "    h[36] = 0\n",
    "    h[38] = 1\n",
    "    h[41] = 2\n",
    "    h[42] = 3\n",
    "    h[43] = 4\n",
    "    h[45] = 5\n",
    "    h[82] = 6\n",
    "\n",
    "    u = list(itertools.repeat(np.zeros(7), max(time)+1))\n",
    "\n",
    "    for i in range(len(notes)):\n",
    "        v = [0]*7\n",
    "        v[h[notes[i]]] = velocity[i] / 127\n",
    "        if (np.any(u[time[i]])):        #If the current vector is not the zero vector\n",
    "            u[time[i]] = u[time[i]] + np.array(v)\n",
    "        else:\n",
    "            u[time[i]] = np.array(v)\n",
    "    \n",
    "    y_train = u[1:]\n",
    "    y_train.append(np.zeros(7))\n",
    "    return np.array(u), np.array(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "  \\mathcal{D}^{-1} : [0,1]^{K \\times n_i} \\rightarrow \\text{CSV}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 875,
   "metadata": {},
   "outputs": [],
   "source": [
    "def D_inv(y):\n",
    "    h_inv = {}\n",
    "    h_inv[0] = 36\n",
    "    h_inv[1] = 38\n",
    "    h_inv[2] = 41\n",
    "    h_inv[3] = 42\n",
    "    h_inv[4] = 43\n",
    "    h_inv[5] = 45\n",
    "    h_inv[6] = 82\n",
    "\n",
    "    dfo = pd.DataFrame([], columns= [\"track\", \"time\", \"note_is_played\"\n",
    "                            ,\"channel\", \"notes\", \"velocity\"])\n",
    "    time_new = []\n",
    "    notes_new = []\n",
    "    velocity_new = []\n",
    "\n",
    "    for n in range(len(y)):\n",
    "        if (np.any(y[n])):\n",
    "            yn = y[n]\n",
    "            for i in range(len(yn)):\n",
    "                v = (int)(yn[i] * 127)\n",
    "                if (v > 0):\n",
    "                    time_new.append(n)\n",
    "                    velocity_new.append(v)\n",
    "                    notes_new.append(h_inv[i])\n",
    "\n",
    "    track_new = [2]*len(time_new)       # arbitrary\n",
    "    note_is_played_new = [\"Note_on_c\"]*len(time_new)\n",
    "    channel_new = [9]*len(time_new)     # 9 for drum\n",
    "\n",
    "    dfo[\"track\"] = track_new\n",
    "    dfo[\"note_is_played\"] = note_is_played_new\n",
    "    dfo[\"channel\"] = channel_new\n",
    "    dfo[\"time\"] = time_new\n",
    "    dfo[\"velocity\"] = velocity_new\n",
    "    dfo[\"notes\"] = notes_new\n",
    "\n",
    "    latestTime = time_new[-1] + 10\n",
    "\n",
    "    pre = [[0,0, \"Header\", 1, 2, 480, ''],\n",
    "            [1, 0, \"Start_track\", '', '', '', ''],\n",
    "            [1, 0, \"Time_signature\", 4, 2, 24, 8],\n",
    "            [1, 0, \"Title_t\", \"\\\"from model\\\"\", '', '', ''],\n",
    "            [1, 0, \"End_track\", '', '', '', ''],\n",
    "            [2, 0, \"Start_track\", '', '', '', '']]\n",
    "\n",
    "    post = [[2, latestTime, \"End_track\", '', '', '', ''],\n",
    "            [0, 0, \"End_of_file\", '', '', '', '']]\n",
    "\n",
    "    dfo[\"filler\"] = ''\n",
    "    # adding pre and post\n",
    "    for i in range(len(pre)):\n",
    "        dfo.loc[i] = pre[i]\n",
    "    for j in range(len(post)):\n",
    "        dfo.loc[len(dfo)+j] = post[j]\n",
    "\n",
    "    dfo.to_csv(\"new.csv\", index = False, header = False)\n",
    "    return dfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TBD: Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 876,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "(7590, 7)\n",
      "(7590, 7)\n"
     ]
    }
   ],
   "source": [
    "columnNames = [\"track\", \"time\", \"note_is_played\"\n",
    "                            , \"channel\", \"notes\", \"velocity\"]\n",
    "\n",
    "df1 = pd.read_csv('drumDemo.csv', skipfooter=2, skiprows=8, engine='python', names= columnNames)\n",
    "df2 = pd.read_csv('drumDemo.csv', skipfooter=2, skiprows=8, engine='python', names= columnNames)\n",
    "df3 = pd.read_csv('drumDemo.csv', skipfooter=2, skiprows=8, engine='python', names= columnNames)\n",
    "df4 = pd.read_csv('drumDemo.csv', skipfooter=2, skiprows=8, engine='python', names= columnNames)\n",
    "dfl = [df1, df2, df3, df4]\n",
    "\n",
    "S = []\n",
    "for df in dfl:\n",
    "    utrain_i, ytrain_i = D(df)\n",
    "    S.append((utrain_i, ytrain_i))\n",
    "\n",
    "print(type(S))\n",
    "u_ex, y_ex = S[0]\n",
    "print(jax.tree_map(lambda x: x.shape, u_ex))\n",
    "print(jax.tree_map(lambda x: x.shape, y_ex))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A RNN (with no LSTM units) is a neural network with recurrent connections (dynamical system)\n",
    "$$\n",
    "\\begin{array}{l l}\n",
    "          \\mathbf{x}(n+1) = \\sigma(W \\mathbf{x}(n) + W^{in}\\mathbf{u}(n+1) + \\mathbf{b}) \\\\\n",
    "          \\mathbf{y}(n) = f(W^{out} \\mathbf{x}(n)),\n",
    "\\end{array}\n",
    "$$\n",
    "Describes how the network activation state is updated and how output signal is generated. \n",
    "\n",
    "Input vector $\\mathbf{u}(n) \\in [0,1]^K$\n",
    "\n",
    "Activation/state vector $\\mathbf{x}(n) \\in \\mathbb{R}^L$\n",
    "\n",
    "Output vector $\\mathbf{y}(n) \\in \\mathbb{R}^K$\n",
    "\n",
    "Bias vector $\\mathbf{b} \\in \\mathbb{R}^L$\n",
    "\n",
    "$W^{in} \\in \\mathbb{R}^{L \\times K}, W \\in \\mathbb{R}^{L \\times L}, W^{out} \\in \\mathbb{R}^{K \\times L}$ are weight matrices charecterizing the connections between neurons in the layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "At time $n = 0$ the recurrent network state $\\mathbf{x}(0)$ is often set to the zero vector $\\mathbf{x}(0) = \\mathbf{0}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 877,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weight_matrix(in_dim, out_dim, key, scale=1e-2):\n",
    "    w = jax.random.normal(key, (out_dim, in_dim))\n",
    "    return scale*w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 878,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_bias(dim, key, scale=1e-2):\n",
    "    b = jax.random.normal(key, (dim, ))\n",
    "    return scale*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 879,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state, params = (W^{in}, W, W^{out}, b)\n",
    "# sizes = (input dim, state dim, output dim.)\n",
    "def init_network(sizes, key):\n",
    "    keys = jax.random.split(key, len(sizes))\n",
    "    # don't know if this is the best way to do it but this is to keep track of the state vector over time\n",
    "    x = []\n",
    "    x.append(np.zeros(sizes[1]))\n",
    "    # as well as output signal\n",
    "    y = []\n",
    "    \n",
    "    Win = init_weight_matrix(sizes[0], sizes[1], keys[0])\n",
    "    W = init_weight_matrix(sizes[1], sizes[1], keys[1])\n",
    "    Wout = init_weight_matrix(sizes[1], sizes[2], keys[3])\n",
    "    b = init_bias(sizes[1], keys[2])\n",
    "\n",
    "    return x, y, (Win, W, Wout, b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 880,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((12, 7), (12, 12), (7, 12), (12,))\n"
     ]
    }
   ],
   "source": [
    "K = 7 # K := input and output vector dim\n",
    "L = 12 # Reservoir or State Vector dim\n",
    "sizes = [K, L, K]\n",
    "x, y, params = init_network(sizes, key)\n",
    "print(jax.tree_map(lambda x: x.shape, params)) # printing shape of network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathbf{x}(n+1) = \\sigma(W \\mathbf{x}(n) + W^{in}\\mathbf{u}(n+1) + \\mathbf{b})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 881,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = weights and bias\n",
    "# u = input signal at time n\n",
    "# x = state vector : returns state at time n\n",
    "# b = bias vector\n",
    "# n = time\n",
    "# changed: the entire state vector and input signal is passed now\n",
    "# adds new state vector to state signal and also returns new state vector\n",
    "def nextState(params, x, u, n):\n",
    "    W_in, W, W_out, b = params\n",
    "\n",
    "    x_new = jax.nn.relu(np.dot(W, x[-1]) + np.dot(W_in, u[n]) + b)\n",
    "    x.append(x_new)\n",
    "    return x_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mathbf{y}(n) = f(W^{out} \\mathbf{x}(n))$$\n",
    "$f$ is a function that ensures the readout $W^{out} \\mathbf{x}(n) \\in \\mathbb{R}^{K} \\mapsto \\mathbf{y}(n) \\in [0,1]^K$. In other words, that the output of the network has vectors whose elements are always between $0$ and $1$:\n",
    "$$\n",
    "       f(x) := \\frac{1}{1+e^{-x}}\n",
    "$$\n",
    "Both $\\sigma$ and $f$ are applied element-wise on a given vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 882,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adds new output vector to output signal and also returns new output vector\n",
    "def readOut(params, x, y):\n",
    "    W_out = params[2]\n",
    "    y_new = jax.nn.sigmoid(np.dot(W_out, x[-1]))\n",
    "    y.append(y_new)\n",
    "    return y_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 883,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_og is initially the zero vector\n",
    "def forward_bp(params, u, x_og=np.zeros((L, ))):\n",
    "    \"\"\" Loop over the time steps of the input sequence\n",
    "    u[n] := [u_0, ..., u_{n_max}] where u_i \\in [0, 1]^K or (K, )\n",
    "    x_og: \\in R^L or (L, )\n",
    "    \"\"\"\n",
    "    Win, W, Wout, b = params\n",
    "    x = x_og.copy()\n",
    "\n",
    "    def apply_fun_scan(params, x, ut):\n",
    "        \"\"\" Perform single step update of the network.\n",
    "        x:  (L, )\n",
    "        un: (K, )\n",
    "        \"\"\"\n",
    "        Win, W, Wout, b = params\n",
    "        x = jax.nn.relu(\n",
    "            np.dot(Win, ut) + np.dot(W, x) + b\n",
    "        )\n",
    "        y = jax.nn.sigmoid(np.dot(Wout, x))\n",
    "        return x, y\n",
    "\n",
    "    f = functools.partial(apply_fun_scan, params)\n",
    "    _, Y = jax.lax.scan(f, x, u)\n",
    "    return Y\n",
    "\n",
    "batch_forward_bp = jax.vmap(forward_bp, in_axes=(None, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function\n",
    "Logistic regression is used for the case where you do not model loudness. Because there it is a categorical task (to hit or not to hit). But if you do model loudness you use the procedure linear regression. Meaning using the loss functions that are mentioned in the RNN section of the reader like MSE or quadratic loss.\n",
    "\n",
    "Time series prediction task $S = (\\mathbf{u}^{(i)}(n), \\mathbf{y}^{(i)}(n))_{i=1, ..., N;n=1, ..., n_i}$ where $\\mathbf{y}^{i}(n) = \\mathbf{u}^{(i)}(n+1)$\n",
    "For now: quadratic loss which is used in stationary tasks\n",
    "$$\n",
    "    L(\\hat{\\mathbf{Y}}_{i, \\theta}^{\\text{train}}, \\mathbf{Y}_i^{\\text{train}}) = \\parallel \\hat{\\mathbf{Y}}_{i, \\theta}^{\\text{train}}, \\mathbf{Y}_i^{\\text{train}} \\parallel^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization\n",
    "$$\n",
    "\\text{reg}(\\theta) = \\sum_{w \\in \\theta} w^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 884,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# could be made nicer\n",
    "def getParameterVector(params):\n",
    "    theta = []\n",
    "    for w in params:\n",
    "        for e in w:\n",
    "            if (e.size > 1):\n",
    "                for i in e:\n",
    "                    theta.append(i)\n",
    "            else:\n",
    "                theta.append(e)\n",
    "    return np.array(theta)\n",
    "\n",
    "def reg(params):\n",
    "    theta = getParameterVector(params)\n",
    "    return np.sum(np.square(theta))\n",
    "\n",
    "def accuracy(params, u, y_true):\n",
    "    true = np.argmax(y_true, axis=1)\n",
    "    pred = np.argmax(batch_forward_bp(params, u), axis=1)\n",
    "    return np.mean(pred == true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathcal{R}^{\\text{emp}}(\\theta) = \\frac{1}{N} \\sum^{N}_{i=1} L(\\hat{\\mathbf{Y}}_{i, \\theta}^{\\text{train}}, \\mathbf{Y}_i^{\\text{train}}) + r^2 \\; \\text{reg}(\\theta)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 885,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(params, u, y_true, r):\n",
    "    y_hat = batch_forward_bp(params, u)\n",
    "    return np.square(np.linalg.norm(np.subtract(y_hat, y_true))) + (r*r)*reg(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "    \\theta^{(n+1)} = \\theta^{(n)} - \\mu \\nabla \\mathcal{R}^{\\text{emp}}(\\theta^{(n)}),\n",
    "$$\n",
    "\n",
    "$$\n",
    "   \\nabla \\mathcal{R}^{\\text{emp}}(\\theta^{(n)}) = \n",
    "\\bigg(\\frac{\\partial  R^{emp}}{\\partial  w_1}(\\theta^{(n)}), ...,\\frac{\\partial  R^{emp}}{\\partial w_D}(\\theta^{(n)}) \\bigg)',\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 886,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def update(params, u, y_true, r, step_size=1e-2):\n",
    "    grads = jax.grad(loss)(params, u, y_true, r)\n",
    "    return [\n",
    "        w - step_size * dw\n",
    "        for w, dw in zip(params, grads)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 887,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nu_train, y_train = S[0]\\nu_batch = []\\nu_batch.append(u_train)\\ny_batch = []\\ny_batch.append(y_train)\\nu_batch = np.array(u_batch)\\ny_batch = np.array(y_batch)\\nprint(u_train.shape)\\nprint(y_train.shape)\\nprint(accuracy(params, u_batch, y_batch))\\n'"
      ]
     },
     "execution_count": 887,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "u_train, y_train = S[0]\n",
    "u_batch = []\n",
    "u_batch.append(u_train)\n",
    "y_batch = []\n",
    "y_batch.append(y_train)\n",
    "u_batch = np.array(u_batch)\n",
    "y_batch = np.array(y_batch)\n",
    "print(u_train.shape)\n",
    "print(y_train.shape)\n",
    "print(accuracy(params, u_batch, y_batch))\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "    \\mathcal{A}(S) = \\theta_{\\text{opt}} = \\underset{\\theta \\in \\Theta}{\\text{argmin}} \\; \\underbrace{\\frac{1}{N} \\sum^N_{i=1} L(\\hat{\\mathbf{Y}}_{\\theta}^{\\text{train}}, \\mathbf{Y}^{\\text{train}})}_{\\mathcal{R}^{\\text{emp}}(\\theta)},\n",
    "$$\n",
    "$$\n",
    "    \\theta^{(n+1)} = \\theta^{(n)} - \\mu \\nabla \\mathcal{R}^{\\text{emp}}(\\theta^{(n)}),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 888,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 (4.97s): train loss 110.39 test loss 27.60| train acc 0.00%   test acc 0.00%  \n",
      "Epoch  1 (4.91s): train loss 110.38 test loss 27.60| train acc 0.00%   test acc 0.00%  \n",
      "Epoch  1 (2.84s): train loss 152.69 test loss 69.93| train acc 0.00%   test acc 0.00%  \n",
      "Epoch  1 (2.88s): train loss 150.99 test loss 68.23| train acc 0.00%   test acc 0.00%  \n",
      "Epoch  1 (2.82s): train loss 248.74 test loss 165.41| train acc 0.00%   test acc 0.00%  \n",
      "Epoch  1 (2.88s): train loss 229.54 test loss 146.35| train acc 0.00%   test acc 0.00%  \n",
      "Epoch  1 (4.03s): "
     ]
    },
    {
     "data": {
      "text/plain": [
       "[DeviceArray([[-0.00119001,  0.01259624, -0.00131898,  0.00153466,\n",
       "                0.01790148, -0.00584567, -0.00977399],\n",
       "              [-0.00944623, -0.00674881, -0.0020011 ,  0.01082675,\n",
       "               -0.00501226, -0.00223849, -0.00982482],\n",
       "              [-0.00129515,  0.00074726, -0.00435336,  0.00055631,\n",
       "               -0.00723396, -0.00704851, -0.01171278],\n",
       "              [ 0.00877953,  0.00424988, -0.00521509, -0.00389891,\n",
       "                0.01553905,  0.0143659 ,  0.01274391],\n",
       "              [ 0.01746623, -0.0012544 ,  0.01016922,  0.01293689,\n",
       "               -0.01183019,  0.01439269, -0.00593833],\n",
       "              [ 0.00969443, -0.00107673,  0.00478851, -0.01253107,\n",
       "                0.00870384, -0.003004  ,  0.00622379],\n",
       "              [-0.01348182, -0.00261646,  0.00646225, -0.00182024,\n",
       "                0.01281928,  0.00430677,  0.00874748],\n",
       "              [ 0.01025784, -0.00033019,  0.00027282, -0.00199228,\n",
       "                0.00259043,  0.00282665,  0.00316758],\n",
       "              [ 0.00980539,  0.00620312,  0.00181049, -0.00056853,\n",
       "               -0.005034  , -0.00329041,  0.00242185],\n",
       "              [ 0.00567748, -0.00226343, -0.00383427, -0.00135377,\n",
       "               -0.0010923 , -0.00806337, -0.00631492],\n",
       "              [-0.01182068,  0.00249124,  0.0003245 , -0.00224993,\n",
       "               -0.01146504, -0.00968383,  0.0017389 ],\n",
       "              [-0.01020129, -0.01901544, -0.01239513, -0.0062877 ,\n",
       "               -0.00279469,  0.00127403, -0.00032276]], dtype=float32),\n",
       " DeviceArray([[-2.97715829e-04,  1.99310714e-03, -1.22627877e-02,\n",
       "                8.80460255e-03, -3.45685473e-03, -6.58789650e-03,\n",
       "               -3.26490472e-03,  5.72483009e-03, -2.07391015e-04,\n",
       "                2.89696432e-03,  6.28785882e-03, -1.41963083e-02],\n",
       "              [ 1.35314581e-03, -6.72436273e-03, -5.21323597e-03,\n",
       "               -5.01284283e-03,  4.86834906e-03, -1.45929866e-03,\n",
       "               -1.17570423e-02, -9.24811792e-03,  3.27932974e-03,\n",
       "                7.63314497e-03, -1.42244464e-02,  2.83163041e-04],\n",
       "              [ 3.44198407e-03,  4.01995750e-03,  7.64916232e-03,\n",
       "               -5.72592765e-03,  1.09360914e-03, -6.26948476e-03,\n",
       "                1.19006885e-02,  9.79506411e-03,  1.21365738e-05,\n",
       "                9.70025940e-05, -1.04270335e-02,  3.77230253e-03],\n",
       "              [ 2.47492315e-03,  4.79199371e-04,  1.21094519e-02,\n",
       "               -3.24200536e-03,  1.55462772e-02,  1.77203008e-04,\n",
       "                2.78456230e-03,  4.06453107e-03,  5.33246296e-03,\n",
       "                2.93503166e-03,  7.03600282e-03, -1.05230454e-02],\n",
       "              [-4.96793818e-03,  6.66066911e-03,  7.50449672e-03,\n",
       "                6.39241375e-03,  2.07103699e-01, -1.01853292e-02,\n",
       "                2.34930851e-02,  1.25033117e-03,  9.39344913e-02,\n",
       "               -7.30523374e-03, -6.49832049e-03, -1.25479815e-03],\n",
       "              [ 1.13323089e-02, -1.01477262e-02, -6.07888540e-03,\n",
       "               -1.42217623e-02, -1.36768389e-02,  5.16029727e-03,\n",
       "                2.87240301e-03, -9.25841648e-03,  1.18135195e-03,\n",
       "                9.23569407e-03,  3.68508929e-03, -2.24820822e-02],\n",
       "              [-8.20162334e-03, -1.98185742e-02, -1.99505780e-02,\n",
       "               -5.50035294e-03,  2.45682267e-03,  9.75305193e-06,\n",
       "               -5.75689226e-03,  1.19987139e-02, -6.77750586e-03,\n",
       "                1.97326430e-04,  1.02444850e-02, -4.05317079e-03],\n",
       "              [-6.47483859e-03,  4.88121947e-03, -2.44617253e-03,\n",
       "                1.65277685e-03,  8.37165862e-03, -1.62625592e-03,\n",
       "                3.97276925e-03, -2.03096308e-03,  1.27324415e-02,\n",
       "                1.01017971e-02,  1.16117962e-03,  1.46808336e-02],\n",
       "              [-5.83495945e-04,  7.31590251e-03, -2.15527834e-03,\n",
       "                1.16756875e-02,  8.14560428e-02, -2.78088031e-03,\n",
       "                5.65896416e-03,  2.47653597e-03,  3.75827290e-02,\n",
       "               -1.27639732e-05, -4.04690113e-03,  1.01951892e-02],\n",
       "              [ 3.24363401e-03,  4.89206193e-03,  5.58411516e-03,\n",
       "                3.02816089e-03, -3.47352065e-02,  1.57600082e-02,\n",
       "               -1.37293180e-02, -1.33868400e-02, -2.75388546e-02,\n",
       "               -2.33951816e-03,  2.52934429e-03,  2.81225820e-03],\n",
       "              [ 1.56426840e-02, -1.49135916e-02, -3.97012662e-03,\n",
       "                1.97871309e-03, -1.76521426e-03, -2.81016482e-03,\n",
       "               -8.40449706e-03, -2.64436705e-03, -4.08822030e-04,\n",
       "                8.29013437e-03,  1.43222855e-02, -8.45617242e-03],\n",
       "              [ 9.26230941e-03, -3.32303508e-03, -7.86553510e-03,\n",
       "                1.36149419e-03, -3.19001637e-02, -1.35414237e-02,\n",
       "                1.30713917e-03, -1.05994837e-02, -9.24428552e-03,\n",
       "               -1.06411688e-02, -1.69830199e-03,  5.90925012e-03]],            dtype=float32),\n",
       " DeviceArray([[ 4.97518922e-04, -2.39731550e-01,  3.40835378e-03,\n",
       "               -5.97662153e-03, -1.25288427e+00,  3.98097979e-03,\n",
       "               -2.16034442e-01,  1.93533972e-02, -4.88037348e-01,\n",
       "               -1.65337808e-02,  4.93208179e-04, -8.55303332e-02],\n",
       "              [ 1.67838931e-02, -2.29959533e-01, -6.59669400e-04,\n",
       "               -1.34124802e-02, -1.27207959e+00,  4.82889591e-03,\n",
       "               -2.04299897e-01,  1.04587702e-02, -4.81528193e-01,\n",
       "               -5.76644950e-03, -5.22112474e-03, -8.63130242e-02],\n",
       "              [-4.58781933e-03, -2.40295455e-01,  1.95371779e-03,\n",
       "               -2.20695650e-03, -1.27639008e+00,  8.41840345e-04,\n",
       "               -2.02823833e-01, -2.08280399e-03, -4.91760820e-01,\n",
       "               -6.95129111e-03,  5.96498651e-03, -7.18850046e-02],\n",
       "              [ 9.43559315e-03, -2.55826831e-01, -7.38643983e-04,\n",
       "               -6.28921296e-03, -1.26085794e+00, -2.41283141e-02,\n",
       "               -1.95794329e-01,  4.61926870e-03, -4.50935811e-01,\n",
       "                1.27271572e-02, -3.27953324e-03, -8.61265287e-02],\n",
       "              [ 1.07684806e-02, -2.54055589e-01,  4.47014580e-03,\n",
       "               -1.24388561e-02, -1.27971625e+00, -3.89332441e-03,\n",
       "               -2.06015930e-01, -8.54860060e-03, -4.77850497e-01,\n",
       "                5.48705924e-03, -1.23987915e-02, -7.39284307e-02],\n",
       "              [-8.71869456e-03, -2.49817938e-01,  3.26976576e-03,\n",
       "               -5.34507073e-03, -1.27971148e+00,  4.01032344e-03,\n",
       "               -1.97295576e-01, -9.00478847e-03, -4.81361270e-01,\n",
       "               -6.58487435e-03,  6.31016353e-03, -7.55045414e-02],\n",
       "              [-1.17397895e-02, -2.35684305e-01,  1.38941780e-02,\n",
       "               -1.04233082e-02, -1.26183379e+00,  2.19828635e-03,\n",
       "               -2.10498944e-01, -3.84984910e-03, -4.78481978e-01,\n",
       "               -4.48328769e-03, -2.22815340e-03, -8.15457031e-02]],            dtype=float32),\n",
       " DeviceArray([-0.00931943, -0.3079623 , -0.00540978, -0.01664208,\n",
       "               3.158054  , -0.00878504, -0.46519497, -0.00876161,\n",
       "               1.5061597 , -1.3184053 , -0.01083381, -1.33154   ],            dtype=float32)]"
      ]
     },
     "execution_count": 888,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# s is a list of tuples [(u1, y1), (u2, y2), ...]\n",
    "def unpack(s):\n",
    "    f1 = map(lambda x: x[0], s)\n",
    "    f2 = map(lambda x: x[1], s)\n",
    "    u_batch = np.array(list(f1))\n",
    "    y_batch = np.array(list(f2))\n",
    "    return (u_batch, y_batch)\n",
    "\n",
    "def train(params, u_train, y_train, u_test, y_test, r, n_epochs=1):\n",
    "    train_acc = []\n",
    "    test_acc = []\n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "    for epoch in range(n_epochs):\n",
    "        start_time = time.time()\n",
    "\n",
    "        params = update(params, u_train, y_train, r)\n",
    "        train_acc.append(accuracy(params, u_train, y_train))\n",
    "        test_acc.append(accuracy(params, u_test, y_test))\n",
    "        train_loss.append(loss(params, u_train, y_train, r))\n",
    "        test_loss.append(loss(params, u_test, y_test, r))\n",
    "\n",
    "        epoch_time = time.time() - start_time\n",
    "\n",
    "        print(f'Epoch {epoch+1:>2} ({epoch_time:<.2f}s): ', end='')\n",
    "        print(f'train loss {train_loss[-1]:<5.2f} test loss {test_loss[-1]:<5.2f}', end='| ')\n",
    "        print(f'train acc {train_acc[-1]:<7.2%} test acc {test_acc[-1]:<7.2%}')\n",
    "    \n",
    "    return params\n",
    "\n",
    "def train2(params, u_train, y_train, r, n_epochs=1):\n",
    "    train_acc = []\n",
    "    train_loss = []\n",
    "    for epoch in range(n_epochs):\n",
    "        start_time = time.time()\n",
    "\n",
    "        params = update(params, u_train, y_train, r)\n",
    "        train_acc.append(accuracy(params, u_train, y_train))\n",
    "        train_loss.append(loss(params, u_train, y_train, r))\n",
    "\n",
    "        epoch_time = time.time() - start_time\n",
    "\n",
    "        print(f'Epoch {epoch+1:>2} ({epoch_time:<.2f}s): ', end='')\n",
    "    return params\n",
    "\n",
    "\n",
    "# given S, params, loss/emprical risk, r (regularization)\n",
    "# Hyperparameters\n",
    "step_size=1e-2\n",
    "k = 2   # k fold cross validation\n",
    "\n",
    "validation_risk_r = []\n",
    "for r in range(0, 3):\n",
    "    validation_risk = []\n",
    "    for j in range(0, k):\n",
    "        V = [S.pop(j)]  # validation set\n",
    "        T = S         # reduced training set\n",
    "        S.insert(j, V[0])\n",
    "        u_train, y_train = unpack(T)\n",
    "        u_val, y_val = unpack(V)\n",
    "        params = train(params, u_train, y_train, u_val, y_val, r)\n",
    "        validation_risk.append(loss(params, u_val, y_val, r))\n",
    "    validation_risk_r.append(np.mean(np.array(validation_risk)))\n",
    "\n",
    "r_opt = np.argmin(np.array(validation_risk_r))\n",
    "u_train, y_train = unpack(V)\n",
    "params = train2(params, u_train, y_train, r_opt)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e375b77a672b76be28c1e80386a6d4db61c866c142c6acff7bc94a65b4573147"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

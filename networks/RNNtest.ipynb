{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Simple) RNN in Jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 854,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as np\n",
    "import math\n",
    "import numpy as onp\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from jax import grad, vmap, jit\n",
    "import jax\n",
    "import jax.numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 855,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre - and Postprocessing\n",
    "$$\n",
    "\\mathcal{D} : \\text{CSV} \\rightarrow [0,1]^{K \\times n_i}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 856,
   "metadata": {},
   "outputs": [],
   "source": [
    "def D(df):\n",
    "    time = df[\"time\"].values\n",
    "    notes = df[\"notes\"].values\n",
    "    velocity = df[\"velocity\"].values\n",
    "    h = {}\n",
    "    h[36] = 0\n",
    "    h[38] = 1\n",
    "    h[41] = 2\n",
    "    h[42] = 3\n",
    "    h[43] = 4\n",
    "    h[45] = 5\n",
    "    h[82] = 6\n",
    "\n",
    "    u = list(itertools.repeat(np.zeros(7), max(time)+1))\n",
    "\n",
    "    for i in range(len(notes)):\n",
    "        v = [0]*7\n",
    "        v[h[notes[i]]] = velocity[i] / 127\n",
    "        if (np.any(u[time[i]])):        #If the current vector is not the zero vector\n",
    "            u[time[i]] = u[time[i]] + np.array(v)\n",
    "        else:\n",
    "            u[time[i]] = np.array(v)\n",
    "    \n",
    "    y_train = u[1:]\n",
    "    y_train.append(np.zeros(7))\n",
    "    return np.array(u), np.array(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "  \\mathcal{D}^{-1} : [0,1]^{K \\times n_i} \\rightarrow \\text{CSV}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 857,
   "metadata": {},
   "outputs": [],
   "source": [
    "def D_inv(y):\n",
    "    h_inv = {}\n",
    "    h_inv[0] = 36\n",
    "    h_inv[1] = 38\n",
    "    h_inv[2] = 41\n",
    "    h_inv[3] = 42\n",
    "    h_inv[4] = 43\n",
    "    h_inv[5] = 45\n",
    "    h_inv[6] = 82\n",
    "\n",
    "    dfo = pd.DataFrame([], columns= [\"track\", \"time\", \"note_is_played\"\n",
    "                            ,\"channel\", \"notes\", \"velocity\"])\n",
    "    time_new = []\n",
    "    notes_new = []\n",
    "    velocity_new = []\n",
    "\n",
    "    for n in range(len(y)):\n",
    "        if (np.any(y[n])):\n",
    "            yn = y[n]\n",
    "            for i in range(len(yn)):\n",
    "                v = (int)(yn[i] * 127)\n",
    "                if (v > 0):\n",
    "                    time_new.append(n)\n",
    "                    velocity_new.append(v)\n",
    "                    notes_new.append(h_inv[i])\n",
    "\n",
    "    track_new = [2]*len(time_new)       # arbitrary\n",
    "    note_is_played_new = [\"Note_on_c\"]*len(time_new)\n",
    "    channel_new = [9]*len(time_new)     # 9 for drum\n",
    "\n",
    "    dfo[\"track\"] = track_new\n",
    "    dfo[\"note_is_played\"] = note_is_played_new\n",
    "    dfo[\"channel\"] = channel_new\n",
    "    dfo[\"time\"] = time_new\n",
    "    dfo[\"velocity\"] = velocity_new\n",
    "    dfo[\"notes\"] = notes_new\n",
    "\n",
    "    latestTime = time_new[-1] + 10\n",
    "\n",
    "    pre = [[0,0, \"Header\", 1, 2, 480, ''],\n",
    "            [1, 0, \"Start_track\", '', '', '', ''],\n",
    "            [1, 0, \"Time_signature\", 4, 2, 24, 8],\n",
    "            [1, 0, \"Title_t\", \"\\\"from model\\\"\", '', '', ''],\n",
    "            [1, 0, \"End_track\", '', '', '', ''],\n",
    "            [2, 0, \"Start_track\", '', '', '', '']]\n",
    "\n",
    "    post = [[2, latestTime, \"End_track\", '', '', '', ''],\n",
    "            [0, 0, \"End_of_file\", '', '', '', '']]\n",
    "\n",
    "    dfo[\"filler\"] = ''\n",
    "    # adding pre and post\n",
    "    for i in range(len(pre)):\n",
    "        dfo.loc[i] = pre[i]\n",
    "    for j in range(len(post)):\n",
    "        dfo.loc[len(dfo)+j] = post[j]\n",
    "\n",
    "    dfo.to_csv(\"new.csv\", index = False, header = False)\n",
    "    return dfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TBD: Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 858,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "(7590, 7)\n",
      "(7590, 7)\n"
     ]
    }
   ],
   "source": [
    "columnNames = [\"track\", \"time\", \"note_is_played\"\n",
    "                            , \"channel\", \"notes\", \"velocity\"]\n",
    "\n",
    "df1 = pd.read_csv('drumDemo.csv', skipfooter=2, skiprows=8, engine='python', names= columnNames)\n",
    "df2 = pd.read_csv('drumDemo.csv', skipfooter=2, skiprows=8, engine='python', names= columnNames)\n",
    "df3 = pd.read_csv('drumDemo.csv', skipfooter=2, skiprows=8, engine='python', names= columnNames)\n",
    "df4 = pd.read_csv('drumDemo.csv', skipfooter=2, skiprows=8, engine='python', names= columnNames)\n",
    "dfl = [df1, df2, df3, df4]\n",
    "\n",
    "S = []\n",
    "for df in dfl:\n",
    "    utrain_i, ytrain_i = D(df)\n",
    "    S.append((utrain_i, ytrain_i))\n",
    "\n",
    "print(type(S))\n",
    "u_ex, y_ex = S[0]\n",
    "print(jax.tree_map(lambda x: x.shape, u_ex))\n",
    "print(jax.tree_map(lambda x: x.shape, y_ex))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A RNN (with no LSTM units) is a neural network with recurrent connections (dynamical system)\n",
    "$$\n",
    "\\begin{array}{l l}\n",
    "          \\mathbf{x}(n+1) = \\sigma(W \\mathbf{x}(n) + W^{in}\\mathbf{u}(n+1) + \\mathbf{b}) \\\\\n",
    "          \\mathbf{y}(n) = f(W^{out} \\mathbf{x}(n)),\n",
    "\\end{array}\n",
    "$$\n",
    "Describes how the network activation state is updated and how output signal is generated. \n",
    "\n",
    "Input vector $\\mathbf{u}(n) \\in [0,1]^K$\n",
    "\n",
    "Activation/state vector $\\mathbf{x}(n) \\in \\mathbb{R}^L$\n",
    "\n",
    "Output vector $\\mathbf{y}(n) \\in \\mathbb{R}^K$\n",
    "\n",
    "Bias vector $\\mathbf{b} \\in \\mathbb{R}^L$\n",
    "\n",
    "$W^{in} \\in \\mathbb{R}^{L \\times K}, W \\in \\mathbb{R}^{L \\times L}, W^{out} \\in \\mathbb{R}^{K \\times L}$ are weight matrices charecterizing the connections between neurons in the layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "At time $n = 0$ the recurrent network state $\\mathbf{x}(0)$ is often set to the zero vector $\\mathbf{x}(0) = \\mathbf{0}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 859,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weight_matrix(in_dim, out_dim, key, scale=1e-2):\n",
    "    w = jax.random.normal(key, (out_dim, in_dim))\n",
    "    return scale*w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 860,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_bias(dim, key, scale=1e-2):\n",
    "    b = jax.random.normal(key, (dim, ))\n",
    "    return scale*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 861,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state, params = (W^{in}, W, W^{out}, b)\n",
    "# sizes = (input dim, state dim, output dim.)\n",
    "def init_network(sizes, key):\n",
    "    keys = jax.random.split(key, len(sizes))\n",
    "    # don't know if this is the best way to do it but this is to keep track of the state vector over time\n",
    "    x = []\n",
    "    x.append(np.zeros(sizes[1]))\n",
    "    # as well as output signal\n",
    "    y = []\n",
    "    \n",
    "    Win = init_weight_matrix(sizes[0], sizes[1], keys[0])\n",
    "    W = init_weight_matrix(sizes[1], sizes[1], keys[1])\n",
    "    Wout = init_weight_matrix(sizes[1], sizes[2], keys[3])\n",
    "    b = init_bias(sizes[1], keys[2])\n",
    "\n",
    "    return x, y, (Win, W, Wout, b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 862,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((12, 7), (12, 12), (7, 12), (12,))\n"
     ]
    }
   ],
   "source": [
    "K = 7 # K := input and output vector dim\n",
    "L = 12 # Reservoir or State Vector dim\n",
    "sizes = [K, L, K]\n",
    "x, y, params = init_network(sizes, key)\n",
    "print(jax.tree_map(lambda x: x.shape, params)) # printing shape of network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathbf{x}(n+1) = \\sigma(W \\mathbf{x}(n) + W^{in}\\mathbf{u}(n+1) + \\mathbf{b})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 863,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = weights and bias\n",
    "# u = input signal at time n\n",
    "# x = state vector : returns state at time n\n",
    "# b = bias vector\n",
    "# n = time\n",
    "# changed: the entire state vector and input signal is passed now\n",
    "# adds new state vector to state signal and also returns new state vector\n",
    "def nextState(params, x, u, n):\n",
    "    W_in, W, W_out, b = params\n",
    "\n",
    "    x_new = jax.nn.relu(np.dot(W, x[-1]) + np.dot(W_in, u[n]) + b)\n",
    "    x.append(x_new)\n",
    "    return x_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mathbf{y}(n) = f(W^{out} \\mathbf{x}(n))$$\n",
    "$f$ is a function that ensures the readout $W^{out} \\mathbf{x}(n) \\in \\mathbb{R}^{K} \\mapsto \\mathbf{y}(n) \\in [0,1]^K$. In other words, that the output of the network has vectors whose elements are always between $0$ and $1$:\n",
    "$$\n",
    "       f(x) := \\frac{1}{1+e^{-x}}\n",
    "$$\n",
    "Both $\\sigma$ and $f$ are applied element-wise on a given vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 864,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adds new output vector to output signal and also returns new output vector\n",
    "def readOut(params, x, y):\n",
    "    W_out = params[2]\n",
    "    y_new = jax.nn.sigmoid(np.dot(W_out, x[-1]))\n",
    "    y.append(y_new)\n",
    "    return y_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 865,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_og is initially the zero vector\n",
    "def forward_bp(params, u, x_og=np.zeros((L, ))):\n",
    "    \"\"\" Loop over the time steps of the input sequence\n",
    "    u[n] := [u_0, ..., u_{n_max}] where u_i \\in [0, 1]^K or (K, )\n",
    "    x_og: \\in R^L or (L, )\n",
    "    \"\"\"\n",
    "    Win, W, Wout, b = params\n",
    "    x = x_og.copy()\n",
    "\n",
    "    def apply_fun_scan(params, x, ut):\n",
    "        \"\"\" Perform single step update of the network.\n",
    "        x:  (L, )\n",
    "        un: (K, )\n",
    "        \"\"\"\n",
    "        Win, W, Wout, b = params\n",
    "        x = jax.nn.relu(\n",
    "            np.dot(Win, ut) + np.dot(W, x) + b\n",
    "        )\n",
    "        y = jax.nn.sigmoid(np.dot(Wout, x))\n",
    "        return x, y\n",
    "\n",
    "    f = functools.partial(apply_fun_scan, params)\n",
    "    _, Y = jax.lax.scan(f, x, u)\n",
    "    return Y\n",
    "\n",
    "batch_forward_bp = jax.vmap(forward_bp, in_axes=(None, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function\n",
    "Logistic regression is used for the case where you do not model loudness. Because there it is a categorical task (to hit or not to hit). But if you do model loudness you use the procedure linear regression. Meaning using the loss functions that are mentioned in the RNN section of the reader like MSE or quadratic loss.\n",
    "\n",
    "Time series prediction task $S = (\\mathbf{u}^{(i)}(n), \\mathbf{y}^{(i)}(n))_{i=1, ..., N;n=1, ..., n_i}$ where $\\mathbf{y}^{i}(n) = \\mathbf{u}^{(i)}(n+1)$\n",
    "For now: quadratic loss which is used in stationary tasks\n",
    "$$\n",
    "    L(\\hat{\\mathbf{Y}}_{i, \\theta}^{\\text{train}}, \\mathbf{Y}_i^{\\text{train}}) = \\parallel \\hat{\\mathbf{Y}}_{i, \\theta}^{\\text{train}}, \\mathbf{Y}_i^{\\text{train}} \\parallel^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization\n",
    "$$\n",
    "\\text{reg}(\\theta) = \\sum_{w \\in \\theta} w^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 866,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# could be made nicer\n",
    "def getParameterVector(params):\n",
    "    theta = []\n",
    "    for w in params:\n",
    "        for e in w:\n",
    "            if (e.size > 1):\n",
    "                for i in e:\n",
    "                    theta.append(i)\n",
    "            else:\n",
    "                theta.append(e)\n",
    "    return np.array(theta)\n",
    "\n",
    "def reg(params):\n",
    "    theta = getParameterVector(params)\n",
    "    return np.sum(np.square(theta))\n",
    "\n",
    "def accuracy(params, u, y_true):\n",
    "    true = np.argmax(y_true, axis=1)\n",
    "    pred = np.argmax(batch_forward_bp(params, u), axis=1)\n",
    "    return np.mean(pred == true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathcal{R}^{\\text{emp}}(\\theta) = \\frac{1}{N} \\sum^{N}_{i=1} L(\\hat{\\mathbf{Y}}_{i, \\theta}^{\\text{train}}, \\mathbf{Y}_i^{\\text{train}}) + r^2 \\; \\text{reg}(\\theta)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 867,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(params, u, y_true, r):\n",
    "    y_hat = batch_forward_bp(params, u)\n",
    "    return np.square(np.linalg.norm(np.subtract(y_hat, y_true))) + (r*r)*reg(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "    \\theta^{(n+1)} = \\theta^{(n)} - \\mu \\nabla \\mathcal{R}^{\\text{emp}}(\\theta^{(n)}),\n",
    "$$\n",
    "\n",
    "$$\n",
    "   \\nabla \\mathcal{R}^{\\text{emp}}(\\theta^{(n)}) = \n",
    "\\bigg(\\frac{\\partial  R^{emp}}{\\partial  w_1}(\\theta^{(n)}), ...,\\frac{\\partial  R^{emp}}{\\partial w_D}(\\theta^{(n)}) \\bigg)',\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 868,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def update(params, u, y_true, r, step_size=1e-2):\n",
    "    grads = jax.grad(loss)(params, u, y_true, r)\n",
    "    return [\n",
    "        w - step_size * dw\n",
    "        for w, dw in zip(params, grads)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 869,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nu_train, y_train = S[0]\\nu_batch = []\\nu_batch.append(u_train)\\ny_batch = []\\ny_batch.append(y_train)\\nu_batch = np.array(u_batch)\\ny_batch = np.array(y_batch)\\nprint(u_train.shape)\\nprint(y_train.shape)\\nprint(accuracy(params, u_batch, y_batch))\\n'"
      ]
     },
     "execution_count": 869,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "u_train, y_train = S[0]\n",
    "u_batch = []\n",
    "u_batch.append(u_train)\n",
    "y_batch = []\n",
    "y_batch.append(y_train)\n",
    "u_batch = np.array(u_batch)\n",
    "y_batch = np.array(y_batch)\n",
    "print(u_train.shape)\n",
    "print(y_train.shape)\n",
    "print(accuracy(params, u_batch, y_batch))\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "    \\mathcal{A}(S) = \\theta_{\\text{opt}} = \\underset{\\theta \\in \\Theta}{\\text{argmin}} \\; \\underbrace{\\frac{1}{N} \\sum^N_{i=1} L(\\hat{\\mathbf{Y}}_{\\theta}^{\\text{train}}, \\mathbf{Y}^{\\text{train}})}_{\\mathcal{R}^{\\text{emp}}(\\theta)},\n",
    "$$\n",
    "$$\n",
    "    \\theta^{(n+1)} = \\theta^{(n)} - \\mu \\nabla \\mathcal{R}^{\\text{emp}}(\\theta^{(n)}),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 870,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 (4.97s): train loss 110.39 test loss 27.60| train acc 0.00%   test acc 0.00%  \n",
      "Epoch  2 (5.29s): train loss 110.38 test loss 27.60| train acc 0.00%   test acc 0.00%  \n",
      "Epoch  3 (3.15s): train loss 110.38 test loss 27.59| train acc 0.00%   test acc 0.00%  \n",
      "Epoch  4 (4.65s): train loss 110.37 test loss 27.59| train acc 0.00%   test acc 0.00%  \n",
      "Epoch  5 (5.39s): train loss 110.36 test loss 27.59| train acc 0.00%   test acc 0.00%  \n",
      "Epoch  6 (3.01s): train loss 110.36 test loss 27.59| train acc 0.00%   test acc 0.00%  \n",
      "Epoch  7 (2.87s): train loss 110.35 test loss 27.59| train acc 0.00%   test acc 0.00%  \n",
      "Epoch  8 (2.95s): train loss 110.35 test loss 27.59| train acc 0.00%   test acc 0.00%  \n",
      "Epoch  9 (3.01s): train loss 110.34 test loss 27.59| train acc 0.00%   test acc 0.00%  \n",
      "Epoch 10 (2.96s): train loss 110.34 test loss 27.58| train acc 0.00%   test acc 0.00%  \n",
      "Epoch  1 (3.04s): train loss 110.34 test loss 27.58| train acc 0.00%   test acc 0.00%  \n",
      "Epoch  2 (3.03s): train loss 110.33 test loss 27.58| train acc 0.00%   test acc 0.00%  \n",
      "Epoch  3 (3.15s): train loss 110.33 test loss 27.58| train acc 0.00%   test acc 0.00%  \n",
      "Epoch  4 (3.08s): train loss 110.33 test loss 27.58| train acc 0.00%   test acc 0.00%  \n",
      "Epoch  5 (3.01s): train loss 110.32 test loss 27.58| train acc 0.00%   test acc 0.00%  \n",
      "Epoch  6 (2.97s): train loss 110.32 test loss 27.58| train acc 0.00%   test acc 0.00%  \n",
      "Epoch  7 (3.20s): train loss 110.32 test loss 27.58| train acc 0.00%   test acc 0.00%  \n",
      "Epoch  8 (3.21s): train loss 110.32 test loss 27.58| train acc 0.00%   test acc 0.00%  \n",
      "Epoch  9 (3.21s): train loss 110.31 test loss 27.58| train acc 0.00%   test acc 0.00%  \n",
      "Epoch 10 (3.29s): train loss 110.31 test loss 27.58| train acc 0.00%   test acc 0.00%  \n",
      "Epoch  1 (3.25s): train loss 110.31 test loss 27.58| train acc 0.00%   test acc 0.00%  \n",
      "Epoch  2 (3.09s): train loss 110.31 test loss 27.58| train acc 0.00%   test acc 0.00%  \n",
      "Epoch  3 (3.30s): train loss 110.30 test loss 27.58| train acc 0.00%   test acc 0.00%  \n",
      "Epoch  4 (3.25s): train loss 110.30 test loss 27.58| train acc 0.00%   test acc 0.00%  \n",
      "Epoch  5 (3.10s): train loss 110.30 test loss 27.58| train acc 0.00%   test acc 0.00%  \n",
      "Epoch  6 (3.22s): train loss 110.30 test loss 27.57| train acc 0.00%   test acc 0.00%  \n",
      "Epoch  7 (3.23s): train loss 110.30 test loss 27.57| train acc 0.00%   test acc 0.00%  \n",
      "Epoch  8 (3.26s): train loss 110.29 test loss 27.57| train acc 0.00%   test acc 0.00%  \n",
      "Epoch  9 (3.29s): train loss 110.29 test loss 27.57| train acc 0.00%   test acc 0.00%  \n",
      "Epoch 10 (3.27s): train loss 110.29 test loss 27.57| train acc 0.00%   test acc 0.00%  \n",
      "Epoch  1 (3.11s): train loss 110.29 test loss 27.57| train acc 0.00%   test acc 0.00%  \n",
      "Epoch  2 (3.07s): train loss 110.29 test loss 27.57| train acc 0.00%   test acc 0.00%  \n",
      "Epoch  3 (3.18s): train loss 110.28 test loss 27.57| train acc 0.00%   test acc 0.00%  \n",
      "Epoch  4 (3.24s): train loss 110.28 test loss 27.57| train acc 0.00%   test acc 0.00%  \n",
      "Epoch  5 (3.40s): train loss 110.28 test loss 27.57| train acc 0.00%   test acc 0.00%  \n",
      "Epoch  6 (3.50s): train loss 110.28 test loss 27.57| train acc 0.00%   test acc 0.00%  \n",
      "Epoch  7 (3.24s): train loss 110.28 test loss 27.57| train acc 0.00%   test acc 0.00%  \n",
      "Epoch  8 (3.17s): train loss 110.28 test loss 27.57| train acc 0.00%   test acc 0.00%  \n",
      "Epoch  9 (3.36s): train loss 110.27 test loss 27.57| train acc 0.00%   test acc 0.00%  \n",
      "Epoch 10 (3.15s): train loss 110.27 test loss 27.57| train acc 0.00%   test acc 0.00%  \n",
      "Epoch  1 (3.14s): train loss 152.33 test loss 69.61| train acc 0.00%   test acc 0.00%  \n",
      "Epoch  2 (3.04s): train loss 150.77 test loss 68.01| train acc 0.00%   test acc 0.00%  \n",
      "Epoch  3 (3.31s): train loss 149.30 test loss 66.52| train acc 0.00%   test acc 0.00%  \n",
      "Epoch  4 (3.03s): train loss 147.89 test loss 65.11| train acc 0.00%   test acc 0.00%  \n",
      "Epoch  5 (3.34s): train loss 146.53 test loss 63.76| train acc 0.00%   test acc 0.00%  \n",
      "Epoch  6 (3.16s): train loss 145.23 test loss 62.46| train acc 0.00%   test acc 0.00%  \n",
      "Epoch  7 (3.01s): train loss 143.97 test loss 61.20| train acc 0.00%   test acc 0.00%  \n",
      "Epoch  8 (2.98s): train loss 142.76 test loss 60.00| train acc 0.00%   test acc 0.00%  \n",
      "Epoch  9 (3.07s): train loss 141.59 test loss 58.83| train acc 0.00%   test acc 0.00%  \n",
      "Epoch 10 (3.12s): train loss 140.46 test loss 57.71| train acc 0.00%   test acc 0.00%  \n",
      "Epoch  1 (3.07s): train loss 139.38 test loss 56.63| train acc 0.00%   test acc 0.00%  \n",
      "Epoch  2 (3.28s): train loss 138.34 test loss 55.59| train acc 0.00%   test acc 0.00%  \n",
      "Epoch  3 (3.35s): train loss 137.33 test loss 54.59| train acc 0.00%   test acc 0.00%  \n",
      "Epoch  4 (3.21s): train loss 136.36 test loss 53.63| train acc 0.00%   test acc 0.00%  \n",
      "Epoch  5 (3.18s): train loss 135.43 test loss 52.70| train acc 0.00%   test acc 0.00%  \n",
      "Epoch  6 (3.12s): train loss 134.54 test loss 51.81| train acc 0.00%   test acc 0.00%  \n",
      "Epoch  7 (3.39s): train loss 133.68 test loss 50.95| train acc 0.00%   test acc 0.00%  \n",
      "Epoch  8 (3.27s): train loss 132.85 test loss 50.12| train acc 0.00%   test acc 0.00%  \n",
      "Epoch  9 (3.31s): train loss 132.06 test loss 49.32| train acc 0.00%   test acc 0.00%  \n",
      "Epoch 10 (3.34s): train loss 131.30 test loss 48.56| train acc 0.00%   test acc 0.00%  \n",
      "Epoch  1 (3.25s): train loss 130.57 test loss 47.82| train acc 0.00%   test acc 0.00%  \n",
      "Epoch  2 (3.36s): train loss 129.87 test loss 47.12| train acc 0.00%   test acc 0.00%  \n",
      "Epoch  3 (3.21s): train loss 129.20 test loss 46.44| train acc 0.00%   test acc 0.00%  \n",
      "Epoch  4 (3.26s): train loss 128.56 test loss 45.79| train acc 0.00%   test acc 0.00%  \n",
      "Epoch  5 (3.37s): train loss 127.95 test loss 45.17| train acc 0.00%   test acc 0.00%  \n",
      "Epoch  6 (3.10s): train loss 127.37 test loss 44.57| train acc 0.00%   test acc 0.00%  \n",
      "Epoch  7 (3.06s): train loss 126.81 test loss 44.01| train acc 0.00%   test acc 0.00%  \n",
      "Epoch  8 (3.06s): train loss 126.29 test loss 43.46| train acc 0.00%   test acc 0.00%  \n",
      "Epoch  9 (3.20s): train loss 125.79 test loss 42.95| train acc 0.00%   test acc 0.00%  \n",
      "Epoch 10 (2.97s): train loss 125.32 test loss 42.44| train acc 0.00%   test acc 0.00%  \n",
      "Epoch  1 (3.00s): train loss 124.90 test loss 42.01| train acc 0.00%   test acc 0.00%  \n",
      "Epoch  2 (3.16s): train loss 124.55 test loss 41.51| train acc 0.00%   test acc 0.00%  \n",
      "Epoch  3 (3.12s): train loss 124.49 test loss 41.40| train acc 0.00%   test acc 0.00%  \n",
      "Epoch  4 (3.19s): train loss 123.91 test loss 40.86| train acc 0.00%   test acc 0.00%  \n",
      "Epoch  5 (3.02s): train loss 123.37 test loss 40.31| train acc 0.00%   test acc 0.00%  \n",
      "Epoch  6 (3.04s): train loss 123.23 test loss 40.11| train acc 0.00%   test acc 0.00%  \n",
      "Epoch  7 (3.03s): train loss 122.75 test loss 39.60| train acc 0.00%   test acc 0.00%  \n",
      "Epoch  8 (3.08s): train loss 122.77 test loss 39.51| train acc 0.00%   test acc 0.00%  \n",
      "Epoch  9 (3.17s): train loss 122.24 test loss 39.05| train acc 0.00%   test acc 0.00%  \n",
      "Epoch 10 (3.02s): train loss 122.52 test loss 38.78| train acc 0.00%   test acc 0.00%  \n",
      "Epoch  1 (3.06s): train loss nan   test loss nan  | train acc 0.00%   test acc 0.00%  \n",
      "Epoch  2 (3.21s): train loss nan   test loss nan  | train acc 0.00%   test acc 0.00%  \n",
      "Epoch  3 (3.07s): train loss nan   test loss nan  | train acc 0.00%   test acc 0.00%  \n",
      "Epoch  4 (3.16s): train loss nan   test loss nan  | train acc 0.00%   test acc 0.00%  \n",
      "Epoch  5 (2.90s): train loss nan   test loss nan  | train acc 0.00%   test acc 0.00%  \n",
      "Epoch  6 (2.96s): train loss nan   test loss nan  | train acc 0.00%   test acc 0.00%  \n",
      "Epoch  7 (3.20s): train loss nan   test loss nan  | train acc 0.00%   test acc 0.00%  \n",
      "Epoch  8 (3.16s): train loss nan   test loss nan  | train acc 0.00%   test acc 0.00%  \n",
      "Epoch  9 (3.08s): train loss nan   test loss nan  | train acc 0.00%   test acc 0.00%  \n",
      "Epoch 10 (3.14s): train loss nan   test loss nan  | train acc 0.00%   test acc 0.00%  \n",
      "Epoch  1 (3.15s): train loss nan   test loss nan  | train acc 0.00%   test acc 0.00%  \n",
      "Epoch  2 (3.11s): train loss nan   test loss nan  | train acc 0.00%   test acc 0.00%  \n",
      "Epoch  3 (3.23s): train loss nan   test loss nan  | train acc 0.00%   test acc 0.00%  \n",
      "Epoch  4 (3.20s): train loss nan   test loss nan  | train acc 0.00%   test acc 0.00%  \n",
      "Epoch  5 (3.04s): train loss nan   test loss nan  | train acc 0.00%   test acc 0.00%  \n",
      "Epoch  6 (3.19s): train loss nan   test loss nan  | train acc 0.00%   test acc 0.00%  \n",
      "Epoch  7 (3.12s): train loss nan   test loss nan  | train acc 0.00%   test acc 0.00%  \n",
      "Epoch  8 (3.71s): train loss nan   test loss nan  | train acc 0.00%   test acc 0.00%  \n",
      "Epoch  9 (3.05s): train loss nan   test loss nan  | train acc 0.00%   test acc 0.00%  \n",
      "Epoch 10 (2.96s): train loss nan   test loss nan  | train acc 0.00%   test acc 0.00%  \n",
      "Epoch  1 (3.04s): train loss nan   test loss nan  | train acc 0.00%   test acc 0.00%  \n",
      "Epoch  2 (3.23s): train loss nan   test loss nan  | train acc 0.00%   test acc 0.00%  \n",
      "Epoch  3 (3.05s): train loss nan   test loss nan  | train acc 0.00%   test acc 0.00%  \n",
      "Epoch  4 (3.12s): train loss nan   test loss nan  | train acc 0.00%   test acc 0.00%  \n",
      "Epoch  5 (3.20s): train loss nan   test loss nan  | train acc 0.00%   test acc 0.00%  \n",
      "Epoch  6 (3.12s): train loss nan   test loss nan  | train acc 0.00%   test acc 0.00%  \n",
      "Epoch  7 (3.12s): train loss nan   test loss nan  | train acc 0.00%   test acc 0.00%  \n",
      "Epoch  8 (3.20s): train loss nan   test loss nan  | train acc 0.00%   test acc 0.00%  \n",
      "Epoch  9 (3.06s): train loss nan   test loss nan  | train acc 0.00%   test acc 0.00%  \n",
      "Epoch 10 (3.13s): train loss nan   test loss nan  | train acc 0.00%   test acc 0.00%  \n",
      "Epoch  1 (3.07s): train loss nan   test loss nan  | train acc 0.00%   test acc 0.00%  \n",
      "Epoch  2 (3.72s): train loss nan   test loss nan  | train acc 0.00%   test acc 0.00%  \n",
      "Epoch  3 (3.64s): train loss nan   test loss nan  | train acc 0.00%   test acc 0.00%  \n",
      "Epoch  4 (3.93s): train loss nan   test loss nan  | train acc 0.00%   test acc 0.00%  \n",
      "Epoch  5 (3.40s): train loss nan   test loss nan  | train acc 0.00%   test acc 0.00%  \n",
      "Epoch  6 (3.21s): train loss nan   test loss nan  | train acc 0.00%   test acc 0.00%  \n",
      "Epoch  7 (3.24s): train loss nan   test loss nan  | train acc 0.00%   test acc 0.00%  \n",
      "Epoch  8 (3.19s): train loss nan   test loss nan  | train acc 0.00%   test acc 0.00%  \n",
      "Epoch  9 (3.14s): train loss nan   test loss nan  | train acc 0.00%   test acc 0.00%  \n",
      "Epoch 10 (4.90s): train loss nan   test loss nan  | train acc 0.00%   test acc 0.00%  \n",
      "Epoch  1 (5.32s): train loss nan   test loss nan  | train acc 0.00%   test acc 0.00%  \n",
      "Epoch  2 (5.20s): train loss nan   test loss nan  | train acc 0.00%   test acc 0.00%  \n",
      "Epoch  3 (5.41s): train loss nan   test loss nan  | train acc 0.00%   test acc 0.00%  \n",
      "Epoch  4 (5.50s): train loss nan   test loss nan  | train acc 0.00%   test acc 0.00%  \n",
      "Epoch  5 (5.35s): train loss nan   test loss nan  | train acc 0.00%   test acc 0.00%  \n",
      "Epoch  6 (4.67s): train loss nan   test loss nan  | train acc 0.00%   test acc 0.00%  \n",
      "Epoch  7 (3.13s): train loss nan   test loss nan  | train acc 0.00%   test acc 0.00%  \n",
      "Epoch  8 (3.45s): train loss nan   test loss nan  | train acc 0.00%   test acc 0.00%  \n"
     ]
    }
   ],
   "source": [
    "# s is a list of tuples [(u1, y1), (u2, y2), ...]\n",
    "def unpack(s):\n",
    "    f1 = map(lambda x: x[0], s)\n",
    "    f2 = map(lambda x: x[1], s)\n",
    "    u_batch = np.array(list(f1))\n",
    "    y_batch = np.array(list(f2))\n",
    "    return (u_batch, y_batch)\n",
    "\n",
    "def train(params, u_train, y_train, u_test, y_test, r, n_epochs=1):\n",
    "    train_acc = []\n",
    "    test_acc = []\n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "    for epoch in range(n_epochs):\n",
    "        start_time = time.time()\n",
    "\n",
    "        params = update(params, u_train, y_train, r)\n",
    "        train_acc.append(accuracy(params, u_train, y_train))\n",
    "        test_acc.append(accuracy(params, u_test, y_test))\n",
    "        train_loss.append(loss(params, u_train, y_train, r))\n",
    "        test_loss.append(loss(params, u_test, y_test, r))\n",
    "\n",
    "        epoch_time = time.time() - start_time\n",
    "\n",
    "        print(f'Epoch {epoch+1:>2} ({epoch_time:<.2f}s): ', end='')\n",
    "        print(f'train loss {train_loss[-1]:<5.2f} test loss {test_loss[-1]:<5.2f}', end='| ')\n",
    "        print(f'train acc {train_acc[-1]:<7.2%} test acc {test_acc[-1]:<7.2%}')\n",
    "    \n",
    "    return params\n",
    "\n",
    "\n",
    "# given S, params, loss/emprical risk, r (regularization)\n",
    "# Hyperparameters\n",
    "step_size=1e-2\n",
    "k = 2   # k fold cross validation\n",
    "\n",
    "validation_risk_r = []\n",
    "for r in range(0, 3):\n",
    "    validation_risk = []\n",
    "    for j in range(0, k):\n",
    "        V = [S.pop(j)]  # validation set\n",
    "        T = S         # reduced training set\n",
    "        S.insert(j, V[0])\n",
    "        u_train, y_train = unpack(T)\n",
    "        u_val, y_val = unpack(V)\n",
    "        params = train(params, u_train, y_train, u_val, y_val, r)\n",
    "        validation_risk.append(loss(params, u_val, y_val, r))\n",
    "    validation_risk_r.append(np.mean(np.array(validation_risk)))\n",
    "\n",
    "print(min(validation_risk_r))\n",
    "print(max(validation_risk_r))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e375b77a672b76be28c1e80386a6d4db61c866c142c6acff7bc94a65b4573147"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Simple) RNN in Jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as np\n",
    "import math\n",
    "import numpy as onp\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from jax import grad, vmap, jit\n",
    "import pandas as pd\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1    2           3  4   5   6\n",
      "0  2    0   Note_on_c  9  42  96\n",
      "1  2    0   Note_on_c  9  36  84\n",
      "2  2   29   Note_on_c  9  36   0\n",
      "3  2   29   Note_on_c  9  42   0\n",
      "4  2  236   Note_on_c  9  38  61\n",
      "{36: 0, 38: 1, 41: 2, 42: 3, 43: 4, 45: 5, 82: 6}\n",
      "(7,)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('drumDemo.csv', skipfooter=2, skiprows=8, engine='python', names=[\"1\", \"2\", \"3\", \"4\", \"5\", \"6\"])\n",
    "print(df.head())\n",
    "\n",
    "track = (df[\"1\"].values)\n",
    "time = (df[\"2\"].values)\n",
    "note_is_played = (df[\"3\"].values)\n",
    "channel = df[\"4\"].values\n",
    "notes = df[\"5\"].values\n",
    "velocity = df[\"6\"].values\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "print(track)\n",
    "print(time)\n",
    "print(note_is_played)\n",
    "print(channel)\n",
    "print(notes)\n",
    "print(velocity)\n",
    "\"\"\"\n",
    "\n",
    "h = {}\n",
    "h[36] = 0\n",
    "h[38] = 1\n",
    "h[41] = 2\n",
    "h[42] = 3\n",
    "h[43] = 4\n",
    "h[45] = 5\n",
    "h[82] = 6\n",
    "print(h)\n",
    "\n",
    "u = list(itertools.repeat(np.zeros(7), max(time)+1))\n",
    "\n",
    "# this is not working correctly yet\n",
    "for i in range(len(notes)):\n",
    "    v = [0]*7\n",
    "    v[h[notes[i]]] = velocity[i] / 127\n",
    "    if (np.any(u[time[i]])):\n",
    "        u[time[i]] = u[time[i]] + np.array(v)\n",
    "    else:\n",
    "        u[time[i]] = np.array(v)\n",
    "\n",
    "print(jax.tree_map(lambda x: x.shape, u[236]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A RNN (with no LSTM units) is a neural network with recurrent connections (dynamical system)\n",
    "$$\n",
    "\\begin{array}{l l}\n",
    "\\pmb{x}(n+1) = \\sigma(W\\pmb{x}(n) + W^{in}\\pmb{u}(n+1) + \\pmb{b}) \\\\\n",
    "\\pmb{y}(n) = f(W^{out} \\pmb{x}(n))\n",
    "\\end{array}\n",
    "$$\n",
    "Describes how the network activation state is updated and how output signal is generated. \n",
    "\n",
    "Input vector $\\pmb{u}(n) \\in \\mathbb{R}^K$\n",
    "\n",
    "Activation/state vector $\\pmb{x}(n) \\in \\mathbb{R}^L$\n",
    "\n",
    "Output vector $\\pmb{y}(n) \\in \\mathbb{R}^M$\n",
    "\n",
    "Bias vector $\\pmb{b} \\in \\mathbb{R}^L$\n",
    "\n",
    "$W^{in} \\in \\mathbb{R}^{L \\times K}, W \\in \\mathbb{R}^{L \\times L}, W^{out} \\in \\mathbb{R}^{M \\times L}$ are weight matrices charecterizing the connections between neurons in the layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "At time $n = 0$ the recurrent network state $\\mathbf{x}(0)$ is often set to the zero vector $\\mathbf{x}(0) = \\mathbf{0}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weight_matrix(in_dim, out_dim, key, scale=1e-2):\n",
    "    w = jax.random.normal(key, (out_dim, in_dim))\n",
    "    return scale*w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_bias(dim, key, scale=1e-2):\n",
    "    b = jax.random.normal(key, (dim, ))\n",
    "    return scale*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state, params = (W^{in}, W, W^{out}, b)\n",
    "# sizes = (input dim, state dim, output dim.)\n",
    "def init_network(sizes, key):\n",
    "    keys = jax.random.split(key, len(sizes))\n",
    "    params = {} # hashmap\n",
    "    # don't know if this is the best way to do it but this is to keep track of the state vector over time\n",
    "    x = []\n",
    "    x.append(np.zeros(sizes[1]))\n",
    "    # as well as output signal\n",
    "    y = []\n",
    "    \n",
    "    params[\"input matrix\"] = init_weight_matrix(sizes[0], sizes[1], keys[0])\n",
    "    params[\"state matrix\"] = init_weight_matrix(sizes[1], sizes[1], keys[1])\n",
    "    params[\"bias vector\"] = init_bias(sizes[1], keys[2])\n",
    "    params[\"output matrix\"] = init_weight_matrix(sizes[1], sizes[2], keys[3])\n",
    "\n",
    "    return x, y, params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bias vector': (12,), 'input matrix': (12, 7), 'output matrix': (7, 12), 'state matrix': (12, 12)}\n"
     ]
    }
   ],
   "source": [
    "sizes = [7, 12, 7]\n",
    "x, y, params = init_network(sizes, key)\n",
    "print(jax.tree_map(lambda x: x.shape, params)) # printing shape of network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\pmb{x}(n) = \\sigma(W\\pmb{x}(n-1) + W^{in}\\pmb{u}(n) + \\pmb{b})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = weights and bias\n",
    "# u = input signal at time n\n",
    "# x = state vector : returns state at time n\n",
    "# b = bias vector\n",
    "# n = time\n",
    "# changed: the entire state vector and input signal is passed now\n",
    "# adds new state vector to state signal and also returns new state vector\n",
    "def nextState(params, x, u, n):\n",
    "    w_in = params[\"input matrix\"]\n",
    "    w = params[\"state matrix\"]\n",
    "    b = params[\"bias vector\"]\n",
    "    x_new = jax.nn.relu(np.dot(w, x[-1]) + np.dot(w_in, u[n]) + b)\n",
    "    x.append(x_new)\n",
    "    return x_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\pmb{y}(n) = f(W^{out} \\pmb{x}(n))$$\n",
    "Softmax makes the output vector a valid probability vector. Given $\\textbf{v} = (v_1, ..., v_d)' \\in \\mathbb{R}^d$:\n",
    "$$\n",
    "f(\\textbf{v}) = \\text{softmax}(\\textbf{v}) = \\frac{1}{Z}(\\exp(v_1), ..., \\exp(v_d))'\n",
    "$$\n",
    "where $Z = \\sum_{i=1, ..., d} \\exp(v_i)$ is the normalization constant.\n",
    "\n",
    "(For now identity function is used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adds new output vector to output signal and also returns new output vector\n",
    "def readOut(params, x, y):\n",
    "    w_out = params[\"output matrix\"]\n",
    "    y_new = np.dot(w_out, x[-1])\n",
    "    y.append(y_new)\n",
    "    return y_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input signal\n",
    "$$\n",
    "\\mathbf{u}(n)_{n=0,..., n_{\\text{max midi file}}} \\in \\mathbb{R}^7\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for n in range(len(u)):\n",
    "    nextState(params, x, u, n)\n",
    "    readOut(params, x, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0.]\n",
      "[0.0000000e+00 3.9604981e-03 0.0000000e+00 0.0000000e+00 2.0270241e-02\n",
      " 0.0000000e+00 3.3458474e-03 0.0000000e+00 7.6153837e-03 7.5484539e-05\n",
      " 0.0000000e+00 1.3498385e-03]\n",
      "[-2.0801499e-04 -3.0292012e-04 -1.0615522e-04 -3.3284104e-04\n",
      " -1.1379622e-04 -1.4905985e-04  9.9559227e-05]\n"
     ]
    }
   ],
   "source": [
    "# some printing\n",
    "# note: u is a jax numpy matrix\n",
    "# x, y are lists whose elements are jax numpy arrays\n",
    "# but u[i], x[i], y[i] are all jax numpy arrays\n",
    "print(u[6000])\n",
    "#print(type(u))\n",
    "#print(type(u[2]))\n",
    "print(x[6000])\n",
    "#print(type(x))\n",
    "#print(type(x[2]))\n",
    "print(y[6000])\n",
    "#print(type(y))\n",
    "#print(type(y[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Time series prediction task $S = (\\mathbf{u}(n), \\mathbf{y}(n))_{n=1, ..., N}$ where $\\mathbf{y}(n) = \\mathbf{u}(n+1)$\n",
    "For now: quadratic loss which is used in stationary tasks\n",
    "$$\n",
    "L(\\hat{\\mathbf{y}}(n), \\mathbf{y}(n)) = \\parallel \\hat{\\mathbf{y}}(n) - \\mathbf{y}(n) \\parallel^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(params, x, y, y_true, n):\n",
    "    y_hat = readOut(params, x, y)\n",
    "    return np.square(np.subtract(y_hat, y_true[n]))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e375b77a672b76be28c1e80386a6d4db61c866c142c6acff7bc94a65b4573147"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Simple) RNN in Jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as np\n",
    "import math\n",
    "import numpy as onp\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from jax import grad, vmap, jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tbc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A RNN (with no LSTM units) is a neural network with recurrent connections (dynamical system)\n",
    "$$\n",
    "\\begin{array}{l l}\n",
    "\\pmb{x}(n+1) = \\sigma(W\\pmb{x}(n) + W^{in}\\pmb{u}(n+1) + \\pmb{b}) \\\\\n",
    "\\pmb{y}(n) = f(W^{out} \\pmb{x}(n))\n",
    "\\end{array}\n",
    "$$\n",
    "Describes how the network activation state is updated and how output signal is generated. \n",
    "\n",
    "Input vector $\\pmb{u}(n) \\in \\mathbb{R}^K$\n",
    "\n",
    "Activation/state vector $\\pmb{x}(n) \\in \\mathbb{R}^L$\n",
    "\n",
    "Output vector $\\pmb{y}(n) \\in \\mathbb{R}^M$\n",
    "\n",
    "Bias vector $\\pmb{b} \\in \\mathbb{R}^L$\n",
    "\n",
    "$W^{in} \\in \\mathbb{R}^{L \\times K}, W \\in \\mathbb{R}^{L \\times L}, W^{out} \\in \\mathbb{R}^{M \\times L}$ are weight matrices charecterizing the connections between neurons in the layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "At time $n = 0$ the recurrent network state $\\mathbf{x}(0)$ is often set to the zero vector $\\mathbf{x}(0) = \\mathbf{0}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.11617039  2.2125063 ]\n",
      "[[-0.11617039]\n",
      " [ 2.2125063 ]]\n"
     ]
    }
   ],
   "source": [
    "print(jax.random.normal(key, (2,)))\n",
    "print(jax.random.normal(key, (2,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weight_matrix(in_dim, out_dim, key, scale=1e-2):\n",
    "    w = jax.random.normal(key, (out_dim, in_dim))\n",
    "    return scale*w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_bias(dim, key, scale=1e-2):\n",
    "    b = jax.random.normal(key, (dim, ))\n",
    "    return scale*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state, params = (W^{in}, W, W^{out}, b)\n",
    "# sizes = (input dim, state dim, output dim.)\n",
    "def init_network(sizes, key):\n",
    "    keys = jax.random.split(key, len(sizes))\n",
    "    params = {} # hashmap\n",
    "    # don't know if this is the best way to do it but this is to keep track of the state vector over time\n",
    "    x = []\n",
    "    x.append(np.zeros(sizes[1]))\n",
    "    # as well as output signal\n",
    "    y = []\n",
    "    \n",
    "    params[\"input matrix\"] = init_weight_matrix(sizes[0], sizes[1], keys[0])\n",
    "    params[\"state matrix\"] = init_weight_matrix(sizes[1], sizes[1], keys[1])\n",
    "    params[\"bias vector\"] = init_bias(sizes[1], keys[2])\n",
    "    params[\"output matrix\"] = init_weight_matrix(sizes[1], sizes[2], keys[3])\n",
    "\n",
    "    return x, y, params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bias vector': (5,), 'input matrix': (5, 3), 'output matrix': (3, 5), 'state matrix': (5, 5)}\n"
     ]
    }
   ],
   "source": [
    "sizes = [3, 5, 3]\n",
    "x, y, params = init_network(sizes, key)\n",
    "print(jax.tree_map(lambda x: x.shape, params)) # printing shape of network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\pmb{x}(n+1) = \\sigma(W\\pmb{x}(n) + W^{in}\\pmb{u}(n+1) + \\pmb{b})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = weights and bias\n",
    "# u = input signal at time n\n",
    "# x = state vector : returns state at time n\n",
    "# b = bias vector\n",
    "# n = time\n",
    "\n",
    "def nextState(params, x, u):\n",
    "    w_in = params[\"input matrix\"]\n",
    "    w = params[\"state matrix\"]\n",
    "    b = params[\"bias vector\"]\n",
    "    x = jax.nn.relu(np.dot(w, x) + np.dot(w_in, u) + b)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\pmb{y}(n) = f(W^{out} \\pmb{x}(n))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readOut(params, x):\n",
    "    # here we just use the identity function for now\n",
    "    w_out = params[\"output matrix\"]\n",
    "    y = np.dot(w_out, x)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example run with random input signal\n",
    "$$\n",
    "\\mathbf{u}(n)_{n=0,..., 20} \\in \\mathbb{R}^3\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = jax.random.normal(jax.random.PRNGKey(2), shape=(20, 3))\n",
    "for n in range(len(u)):\n",
    "    x.append(nextState(params, x[-1], u[n]))\n",
    "    y.append(readOut(params, x[-1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.4072965 -0.5142992  0.7693824]\n",
      "<class 'jaxlib.xla_extension.DeviceArray'>\n",
      "<class 'jaxlib.xla_extension.DeviceArray'>\n",
      "[5.4300297e-05 3.8219169e-03 0.0000000e+00 3.5909493e-02 0.0000000e+00]\n",
      "<class 'list'>\n",
      "<class 'jaxlib.xla_extension.DeviceArray'>\n",
      "[2.6130828e-04 4.6637055e-05 1.4594362e-04]\n",
      "<class 'list'>\n",
      "<class 'jaxlib.xla_extension.DeviceArray'>\n"
     ]
    }
   ],
   "source": [
    "# some printing\n",
    "# note: u is a jax numpy matrix\n",
    "# x, y are lists whose elements are jax numpy arrays\n",
    "# but u[i], x[i], y[i] are all jax numpy arrays\n",
    "print(u[2])\n",
    "print(type(u))\n",
    "print(type(u[2]))\n",
    "print(x[2])\n",
    "print(type(x))\n",
    "print(type(x[2]))\n",
    "print(y[2])\n",
    "print(type(y))\n",
    "print(type(y[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(params, u, y_true):\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e375b77a672b76be28c1e80386a6d4db61c866c142c6acff7bc94a65b4573147"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

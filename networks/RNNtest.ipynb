{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Simple) RNN in Jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as np\n",
    "import math\n",
    "import numpy as onp\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from jax import grad, vmap, jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tbc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A RNN (with no LSTM units) is a neural network with recurrent connections (dynamical system)\n",
    "$$\n",
    "\\begin{array}{l l}\n",
    "\\pmb{x}(n+1) = \\sigma(W\\pmb{x}(n) + W^{in}\\pmb{u}(n+1) + \\pmb{b}) \\\\\n",
    "\\pmb{y}(n) = f(W^{out} \\pmb{x}(n))\n",
    "\\end{array}\n",
    "$$\n",
    "Describes how the network activation state is updated and how output signal is generated. \n",
    "\n",
    "Input vector $\\pmb{u}(n) \\in \\mathbb{R}^K$\n",
    "\n",
    "Activation/state vector $\\pmb{x}(n) \\in \\mathbb{R}^L$\n",
    "\n",
    "Output vector $\\pmb{y}(n) \\in \\mathbb{R}^M$\n",
    "\n",
    "Bias vector $\\pmb{b} \\in \\mathbb{R}^L$\n",
    "\n",
    "$W^{in} \\in \\mathbb{R}^{L \\times K}, W \\in \\mathbb{R}^{L \\times L}, W^{out} \\in \\mathbb{R}^{M \\times L}$ are weight matrices charecterizing the connections between neurons in the layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "At time $n = 0$ the recurrent network state $\\mathbf{x}(0)$ is often set to the zero vector $\\mathbf{x}(0) = \\mathbf{0}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weight_matrix(in_dim, out_dim, key, scale=1e-2):\n",
    "    w = jax.random.normal(key, (out_dim, in_dim))\n",
    "    return scale*w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_bias(dim, key, scale=1e-2):\n",
    "    b = jax.random.normal(key, (dim, ))\n",
    "    return scale*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state, params = (W^{in}, W, W^{out}, b)\n",
    "# sizes = (input dim, state dim, output dim.)\n",
    "def init_network(sizes, key):\n",
    "    keys = jax.random.split(key, len(sizes))\n",
    "    params = {} # hashmap\n",
    "    # don't know if this is the best way to do it but this is to keep track of the state vector over time\n",
    "    x = []\n",
    "    x.append(np.zeros(sizes[1]))\n",
    "    # as well as output signal\n",
    "    y = []\n",
    "    \n",
    "    params[\"input matrix\"] = init_weight_matrix(sizes[0], sizes[1], keys[0])\n",
    "    params[\"state matrix\"] = init_weight_matrix(sizes[1], sizes[1], keys[1])\n",
    "    params[\"bias vector\"] = init_bias(sizes[1], keys[2])\n",
    "    params[\"output matrix\"] = init_weight_matrix(sizes[1], sizes[2], keys[3])\n",
    "\n",
    "    return x, y, params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bias vector': (5,), 'input matrix': (5, 3), 'output matrix': (3, 5), 'state matrix': (5, 5)}\n"
     ]
    }
   ],
   "source": [
    "sizes = [3, 5, 3]\n",
    "x, y, params = init_network(sizes, key)\n",
    "print(jax.tree_map(lambda x: x.shape, params)) # printing shape of network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\pmb{x}(n) = \\sigma(W\\pmb{x}(n-1) + W^{in}\\pmb{u}(n) + \\pmb{b})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = weights and bias\n",
    "# u = input signal at time n\n",
    "# x = state vector : returns state at time n\n",
    "# b = bias vector\n",
    "# n = time\n",
    "# changed: the entire state vector and input signal is passed now\n",
    "# adds new state vector to state signal and also returns new state vector\n",
    "def nextState(params, x, u, n):\n",
    "    w_in = params[\"input matrix\"]\n",
    "    w = params[\"state matrix\"]\n",
    "    b = params[\"bias vector\"]\n",
    "    x_new = jax.nn.relu(np.dot(w, x[-1]) + np.dot(w_in, u[n]) + b)\n",
    "    x.append(x_new)\n",
    "    return x_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\pmb{y}(n) = f(W^{out} \\pmb{x}(n))$$\n",
    "Softmax makes the output vector a valid probability vector. Given $\\textbf{v} = (v_1, ..., v_d)' \\in \\mathbb{R}^d$:\n",
    "$$\n",
    "f(\\textbf{v}) = \\text{softmax}(\\textbf{v}) = \\frac{1}{Z}(\\exp(v_1), ..., \\exp(v_d))'\n",
    "$$\n",
    "where $Z = \\sum_{i=1, ..., d} \\exp(v_i)$ is the normalization constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adds new output vector to output signal and also returns new output vector\n",
    "def readOut(params, x, y):\n",
    "    w_out = params[\"output matrix\"]\n",
    "    y_new = jax.nn.softmax(np.dot(w_out, x[-1]))\n",
    "    y.append(y_new)\n",
    "    return y_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example run with random input signal\n",
    "$$\n",
    "\\mathbf{u}(n)_{n=0,..., 20} \\in \\mathbb{R}^3\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = jax.random.normal(jax.random.PRNGKey(2), shape=(20, 3))\n",
    "for n in range(len(u)):\n",
    "    nextState(params, x, u, n)\n",
    "    readOut(params, x, y)\n",
    "    #x.append(nextState(params, x, u, n))\n",
    "    #y.append(readOut(params, x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.4072965 -0.5142992  0.7693824]\n",
      "<class 'jaxlib.xla_extension.DeviceArray'>\n",
      "<class 'jaxlib.xla_extension.DeviceArray'>\n",
      "[5.4300297e-05 3.8219169e-03 0.0000000e+00 3.5909493e-02 0.0000000e+00]\n",
      "<class 'list'>\n",
      "<class 'jaxlib.xla_extension.DeviceArray'>\n",
      "[0.33337    0.33329844 0.33333156]\n",
      "<class 'list'>\n",
      "<class 'jaxlib.xla_extension.DeviceArray'>\n"
     ]
    }
   ],
   "source": [
    "# some printing\n",
    "# note: u is a jax numpy matrix\n",
    "# x, y are lists whose elements are jax numpy arrays\n",
    "# but u[i], x[i], y[i] are all jax numpy arrays\n",
    "print(u[2])\n",
    "print(type(u))\n",
    "print(type(u[2]))\n",
    "print(x[2])\n",
    "print(type(x))\n",
    "print(type(x[2]))\n",
    "print(y[2])\n",
    "print(type(y))\n",
    "print(type(y[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Time series prediction task $S = (\\mathbf{u}(n), \\mathbf{y}(n))_{n=1, ..., N}$ where $\\mathbf{y}(n) = \\mathbf{u}(n+1)$\n",
    "For now: quadratic loss which is used in stationary tasks\n",
    "$$\n",
    "L(\\hat{\\mathbf{y}}(n), \\mathbf{y}(n)) = \\parallel \\hat{\\mathbf{y}}(n) - \\mathbf{y}(n) \\parallel^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.44441688 0.44447714 0.4444393 ]\n"
     ]
    }
   ],
   "source": [
    "def loss(params, x, y, y_true, n):\n",
    "    y_hat = readOut(params, x, y)\n",
    "    return np.square(np.subtract(y_hat, y_true[n]))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e375b77a672b76be28c1e80386a6d4db61c866c142c6acff7bc94a65b4573147"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

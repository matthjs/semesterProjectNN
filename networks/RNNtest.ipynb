{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN in Jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as np\n",
    "import math\n",
    "import numpy as onp\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from jax import grad, vmap, jit\n",
    "import jax\n",
    "import jax.numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import functools\n",
    "import os\n",
    "from shutil import copy\n",
    "from shutil import move\n",
    "import pickle\n",
    "from jax.config import config\n",
    "config.update(\"jax_debug_nans\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "TRAINING_ON = True\n",
    "PROCESSING_AND_DATA_LOADING_ON = True\n",
    "SAVE_PARAMETERS = True\n",
    "READ_PARAMETERS = False\n",
    "RC_ON = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre - and Postprocessing\n",
    "$$\n",
    "\\mathcal{D} : \\text{CSV} \\rightarrow [0,1]^{K \\times n_i}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Given a dataframe of a drum track and a hashmap\n",
    "for mapping the drum kits to an index of the vector.\n",
    "Returns the vectorized training set\n",
    "consisting of the training input and output.\n",
    "maxTime is for zero padding. If a signal is shorter than maxTime\n",
    "then it is zero padded\n",
    "\"\"\"\n",
    "def D(df, h, maxTime):\n",
    "\n",
    "    time = df[\"time\"].values\n",
    "    notes = df[\"notes\"].values\n",
    "    velocity = df[\"velocity\"].values\n",
    "\n",
    "    dim = len(h)\n",
    "    u = list(itertools.repeat(np.zeros(dim), max(time)+1))\n",
    "\n",
    "    for i in range(len(notes)):\n",
    "        v = [0]*dim\n",
    "        v[h[int(notes[i])]] = int(velocity[i]) / 127\n",
    "        if (np.any(u[time[i]])):        #If the current vector is not the zero vector\n",
    "            u[time[i]] = u[time[i]] + np.array(v)\n",
    "        else:\n",
    "            u[time[i]] = np.array(v)\n",
    "    \n",
    "    y_train = u[1:]\n",
    "    y_train.append(np.zeros(dim))\n",
    "\n",
    "    padding = maxTime - max(time)\n",
    "    return np.pad(np.array(u), ((0, padding), (0, 0)), \"constant\"), np.pad(np.array(y_train), ((0, padding), (0, 0)), \"constant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "  \\mathcal{D}^{-1} : [0,1]^{K \\times n_i} \\rightarrow \\text{CSV}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Given an output vector and a hashmap\n",
    "for mapping indices of the vector to the\n",
    "appropriate note/drum kit. Returns a dataframe\n",
    "that can be converted to a csv file and then to a midi file\n",
    "\"\"\"\n",
    "def D_inv(y, h_inv):\n",
    "    dfo = pd.DataFrame([], columns= [\"track\", \"time\", \"control\"\n",
    "                            ,\"channel\", \"notes\", \"velocity\"])\n",
    "    time_new = []\n",
    "    notes_new = []\n",
    "    velocity_new = []\n",
    "\n",
    "    for n in range(len(y)):\n",
    "        if (np.any(y[n])):\n",
    "            yn = y[n]\n",
    "            for i in range(len(yn)):\n",
    "                v = (int)(yn[i] * 127)\n",
    "                if (v > 0):\n",
    "                    time_new.append(n)\n",
    "                    velocity_new.append(v)\n",
    "                    notes_new.append(h_inv[i])\n",
    "\n",
    "    track_new = [2]*len(time_new)       # arbitrary\n",
    "    note_is_played_new = [\"Note_on_c\"]*len(time_new)\n",
    "    channel_new = [9]*len(time_new)     # 9 for drum\n",
    "\n",
    "    dfo[\"track\"] = track_new\n",
    "    dfo[\"note_is_played\"] = note_is_played_new\n",
    "    dfo[\"channel\"] = channel_new\n",
    "    dfo[\"time\"] = time_new\n",
    "    dfo[\"velocity\"] = velocity_new\n",
    "    dfo[\"notes\"] = notes_new\n",
    "\n",
    "    latestTime = time_new[-1] + 10\n",
    "\n",
    "    pre = [[0,0, \"Header\", 1, 2, 480, ''],\n",
    "            [1, 0, \"Start_track\", '', '', '', ''],\n",
    "            [1, 0, \"Time_signature\", 4, 2, 24, 8],\n",
    "            [1, 0, \"Title_t\", \"\\\"from model\\\"\", '', '', ''],\n",
    "            [1, 0, \"End_track\", '', '', '', ''],\n",
    "            [2, 0, \"Start_track\", '', '', '', '']]\n",
    "\n",
    "    post = [[2, latestTime, \"End_track\", '', '', '', ''],\n",
    "            [0, 0, \"End_of_file\", '', '', '', '']]\n",
    "\n",
    "    dfo[\"filler\"] = ''\n",
    "    # adding pre and post\n",
    "    for i in range(len(pre)):\n",
    "        dfo.loc[i] = pre[i]\n",
    "    for j in range(len(post)):\n",
    "        dfo.loc[len(dfo)+j] = post[j]\n",
    "\n",
    "    #dfo.to_csv(\"new.csv\", index = False, header = False)\n",
    "    return dfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Returns a list of dataframes for each track\n",
    "seperated by \"start track\" and \"end track\"\n",
    "\"\"\"\n",
    "def getTracks(df):\n",
    "    subDataFrames = []\n",
    "    startingIdx = -1\n",
    "    # get each track as seperate dataframe\n",
    "    for idx, row in df.iterrows():\n",
    "        if (row[\"control\"] == \" Start_track\"):\n",
    "            startingIdx = idx\n",
    "        elif (row[\"control\"] == \" End_track\"):\n",
    "            subDataFrames.append(df[startingIdx : idx + 1])\n",
    "    return subDataFrames\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Given a dataFrame converted from a MIDI file\n",
    "returns a list of dataFrames of just the channel 9 tracks\n",
    "and removes all rows where the control column is\n",
    "not \"note_on_c\"\n",
    "i.e. the drum tracks\n",
    "Meaning that if a song has multiple drum tracks the list\n",
    "will contain more than one dataframe.\n",
    "\"\"\"\n",
    "def getDrumTracks(df):\n",
    "    # get those tracks that are drum tracks\n",
    "    drumDataFrames = [d[d[\"control\"] == \" Note_on_c\"] for d in getTracks(df) if \" 9\" in list(d[\"channel\"])]\n",
    "    return drumDataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "    \\mathcal{M} : \\text{MIDI} \\rightarrow \\text{CSV}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv file already there\n",
      "csv file already there\n",
      "csv file already there\n",
      "csv file already there\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "Copies over the MIDI files to the folder \n",
    "where the can be converted.\n",
    "\n",
    "\"\"\"\n",
    "cwd = os.getcwd()\n",
    "\n",
    "os.chdir('../midi/NetworkInputMIDI')\n",
    "\n",
    "fileNames = []\n",
    "for m in os.listdir():\n",
    "    fileNames.append(m)\n",
    "    copy(m, '../midicsv-1.1')\n",
    "\n",
    "os.chdir('../midicsv-1.1')\n",
    "for m in fileNames:\n",
    "    command = \"midicsv\" + \" \" + m + \" \" + m[:-4] + \".csv\"\n",
    "    res = os.system(command)\n",
    "    os.remove(m)\n",
    "    try:\n",
    "        move(m[:-4] + \".csv\", '../NetworkInputCSV')\n",
    "    except:\n",
    "        print(\"csv file already there\")\n",
    "        os.remove(m[:-4] + \".csv\")\n",
    "\n",
    "# ensures that the cwd resets\n",
    "os.chdir(cwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now import the csv files as panda dataframes (get just the drum tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../midi/NetworkInputCSV')\n",
    "\n",
    "columnNames = [\"track\", \"time\", \"control\"\n",
    "                                , \"channel\", \"notes\", \"velocity\"]\n",
    "\n",
    "\"\"\"\n",
    "Loop through all the csv files in\n",
    "the network input folder\n",
    "\"\"\"\n",
    "drumTracks_df = []\n",
    "for csv in os.listdir():\n",
    "    # for now: generalize later\n",
    "    df = pd.read_csv(csv, skiprows=6, engine='python', names= columnNames)\n",
    "    df.dropna()\n",
    "    drumTracks_df = drumTracks_df + getDrumTracks(df)\n",
    "\n",
    "os.chdir(cwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create hashmap that maps notes to right vector index and additionally\n",
    "1. normalize time\n",
    "2. get max time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[     track  time     control channel  notes  velocity\n",
      "2        2     0   Note_on_c       9     42      96.0\n",
      "3        2     0   Note_on_c       9     36      84.0\n",
      "4        2    29   Note_on_c       9     36       0.0\n",
      "5        2    29   Note_on_c       9     42       0.0\n",
      "6        2   236   Note_on_c       9     38      61.0\n",
      "..     ...   ...         ...     ...    ...       ...\n",
      "163      2  7436   Note_on_c       9     41      83.0\n",
      "164      2  7469   Note_on_c       9     42       0.0\n",
      "165      2  7469   Note_on_c       9     41       0.0\n",
      "166      2  7556   Note_on_c       9     38      43.0\n",
      "167      2  7589   Note_on_c       9     38       0.0\n",
      "\n",
      "[166 rows x 6 columns],      track  time     control channel  notes  velocity\n",
      "2        2     0   Note_on_c       9     42      96.0\n",
      "3        2     0   Note_on_c       9     36      84.0\n",
      "4        2    29   Note_on_c       9     36       0.0\n",
      "5        2    29   Note_on_c       9     42       0.0\n",
      "6        2   236   Note_on_c       9     38      61.0\n",
      "..     ...   ...         ...     ...    ...       ...\n",
      "163      2  7436   Note_on_c       9     41      83.0\n",
      "164      2  7469   Note_on_c       9     42       0.0\n",
      "165      2  7469   Note_on_c       9     41       0.0\n",
      "166      2  7556   Note_on_c       9     38      43.0\n",
      "167      2  7589   Note_on_c       9     38       0.0\n",
      "\n",
      "[166 rows x 6 columns],      track  time     control channel  notes  velocity\n",
      "2        2     0   Note_on_c       9     36      84.0\n",
      "3        2     0   Note_on_c       9     51      96.0\n",
      "4        2    29   Note_on_c       9     36       0.0\n",
      "5        2    29   Note_on_c       9     51       0.0\n",
      "6        2   476   Note_on_c       9     39      96.0\n",
      "..     ...   ...         ...     ...    ...       ...\n",
      "153      2  7229   Note_on_c       9     39       0.0\n",
      "154      2  7316   Note_on_c       9     82      53.0\n",
      "155      2  7349   Note_on_c       9     82       0.0\n",
      "156      2  7556   Note_on_c       9     39      53.0\n",
      "157      2  7589   Note_on_c       9     39       0.0\n",
      "\n",
      "[156 rows x 6 columns],      track  time     control channel  notes  velocity\n",
      "2        2     0   Note_on_c       9     41      86.0\n",
      "3        2     0   Note_on_c       9     36      84.0\n",
      "4        2     0   Note_on_c       9     51      96.0\n",
      "5        2    29   Note_on_c       9     51       0.0\n",
      "6        2    29   Note_on_c       9     41       0.0\n",
      "..     ...   ...         ...     ...    ...       ...\n",
      "185      2  7436   Note_on_c       9     53      71.0\n",
      "186      2  7469   Note_on_c       9     36       0.0\n",
      "187      2  7469   Note_on_c       9     53       0.0\n",
      "188      2  7556   Note_on_c       9     53      53.0\n",
      "189      2  7589   Note_on_c       9     53       0.0\n",
      "\n",
      "[188 rows x 6 columns]]\n",
      "{36: 0, 37: 1, 38: 2, 39: 3, 41: 4, 42: 5, 43: 6, 45: 7, 48: 8, 51: 9, 53: 10, 80: 11, 81: 12, 82: 13}\n",
      "{0: 36, 1: 37, 2: 38, 3: 39, 4: 41, 5: 42, 6: 43, 7: 45, 8: 48, 9: 51, 10: 53, 11: 80, 12: 81, 13: 82}\n"
     ]
    }
   ],
   "source": [
    "h = {}\n",
    "h_inv = {}\n",
    "unique_notes = set()\n",
    "\n",
    "#maxTime = -1    # for zero padding later\n",
    "\n",
    "normalizingFactorFound = False\n",
    "for d in drumTracks_df:\n",
    "    d[\"notes\"] = d[\"notes\"].transform(lambda x : int(x))\n",
    "    unique_notes = unique_notes.union(set(d[\"notes\"]))\n",
    "    normTime = d.iloc[0][\"time\"]\n",
    "    d[\"time\"] = d[\"time\"].transform(lambda x : x - normTime)\n",
    "\n",
    "    \"\"\"\n",
    "    for idx, row in d.iterrows():\n",
    "        if (row[\"time\"] > maxTime):\n",
    "            maxTime = row[\"time\"]\n",
    "    \"\"\"\n",
    "print(drumTracks_df)\n",
    "notes = list(unique_notes)\n",
    "notes.sort()\n",
    "\n",
    "for idx in range(0, len(notes)):\n",
    "    h[notes[idx]] = idx\n",
    "    h_inv[idx] = notes[idx]        # make sure they are integers and not strings\n",
    "\n",
    "print(h)\n",
    "print(h_inv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seperate drum tracks into bars every 2222.222 ms is a bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "0\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "def seperateIntoBars(df, barLength=2222):\n",
    "    subDataFrames = []\n",
    "    startingTime = 0\n",
    "    startingIdx = 0\n",
    "    for idx, row in df.iterrows():\n",
    "        if (row[\"time\"] - startingTime > barLength):\n",
    "            subDataFrames.append(df[startingIdx : idx + 1].copy())\n",
    "            startingTime = row[\"time\"]\n",
    "            startingIdx = idx\n",
    "    \n",
    "    for d in subDataFrames:\n",
    "        normTime = d.iloc[0][\"time\"]\n",
    "        d[\"time\"] = d[\"time\"].transform(lambda x : x - normTime)\n",
    "    return subDataFrames\n",
    "\n",
    "df_bars = []\n",
    "for df in drumTracks_df:\n",
    "    df_bars = df_bars + seperateIntoBars(df)\n",
    "\n",
    "print(len(drumTracks_df))\n",
    "print(len(df_bars))\n",
    "\n",
    "print(df_bars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max time\n",
    "maxTime = -1\n",
    "for d in df_bars:\n",
    "    for idx, row in d.iterrows():\n",
    "        if (row[\"time\"] > maxTime):\n",
    "            maxTime = row[\"time\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get vectors from drum track dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Matth\\OneDrive\\Documenten\\NN\\semester-project\\networks\\RNNtest.ipynb Cell 20'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Matth/OneDrive/Documenten/NN/semester-project/networks/RNNtest.ipynb#ch0000016?line=5'>6</a>\u001b[0m     u_train, y_train \u001b[39m=\u001b[39m D(df, h, maxTime)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Matth/OneDrive/Documenten/NN/semester-project/networks/RNNtest.ipynb#ch0000016?line=6'>7</a>\u001b[0m     S\u001b[39m.\u001b[39mappend((u_train, y_train))\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Matth/OneDrive/Documenten/NN/semester-project/networks/RNNtest.ipynb#ch0000016?line=8'>9</a>\u001b[0m u_ex, y_ex \u001b[39m=\u001b[39m S[\u001b[39m0\u001b[39;49m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Matth/OneDrive/Documenten/NN/semester-project/networks/RNNtest.ipynb#ch0000016?line=9'>10</a>\u001b[0m \u001b[39mprint\u001b[39m(jax\u001b[39m.\u001b[39mtree_map(\u001b[39mlambda\u001b[39;00m x: x\u001b[39m.\u001b[39mshape, u_ex))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Matth/OneDrive/Documenten/NN/semester-project/networks/RNNtest.ipynb#ch0000016?line=10'>11</a>\u001b[0m \u001b[39mprint\u001b[39m(jax\u001b[39m.\u001b[39mtree_map(\u001b[39mlambda\u001b[39;00m x: x\u001b[39m.\u001b[39mshape, y_ex))\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "if (PROCESSING_AND_DATA_LOADING_ON):\n",
    "    S = []  # S will be a list of tuples\n",
    "            # each tuple contains two input signals over time\n",
    "\n",
    "    for df in df_bars:\n",
    "        u_train, y_train = D(df, h, maxTime)\n",
    "        S.append((u_train, y_train))\n",
    "\n",
    "    u_ex, y_ex = S[0]\n",
    "    print(jax.tree_map(lambda x: x.shape, u_ex))\n",
    "    print(jax.tree_map(lambda x: x.shape, y_ex))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save vectors so we do not have to rerun it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pynative.com/python-write-list-to-file/\n",
    "# write list to binary file\n",
    "def write_list(a_list):\n",
    "    # store list in binary file so 'wb' mode\n",
    "    with open('listfile', 'wb') as fp:\n",
    "        pickle.dump(a_list, fp)\n",
    "        print('Done writing list into a binary file')\n",
    "\n",
    "# Read list to memory\n",
    "def read_list():\n",
    "    # for reading also binary mode is important\n",
    "    with open('listfile', 'rb') as fp:\n",
    "        n_list = pickle.load(fp)\n",
    "        return n_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done writing list into a binary file\n"
     ]
    }
   ],
   "source": [
    "if (PROCESSING_AND_DATA_LOADING_ON):\n",
    "    write_list(S)\n",
    "else:\n",
    "    S = read_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A RNN (with no LSTM units) is a neural network with recurrent connections (dynamical system)\n",
    "$$\n",
    "\\begin{array}{l l}\n",
    "          \\mathbf{x}(n+1) = \\sigma(W \\mathbf{x}(n) + W^{in}\\mathbf{u}(n+1) + \\mathbf{b}) \\\\\n",
    "          \\mathbf{y}(n) = f(W^{out} \\mathbf{x}(n)),\n",
    "\\end{array}\n",
    "$$\n",
    "Describes how the network activation state is updated and how output signal is generated. \n",
    "\n",
    "Input vector $\\mathbf{u}(n) \\in [0,1]^K$\n",
    "\n",
    "Activation/state vector $\\mathbf{x}(n) \\in \\mathbb{R}^L$\n",
    "\n",
    "Output vector $\\mathbf{y}(n) \\in \\mathbb{R}^K$\n",
    "\n",
    "Bias vector $\\mathbf{b} \\in \\mathbb{R}^L$\n",
    "\n",
    "$W^{in} \\in \\mathbb{R}^{L \\times K}, W \\in \\mathbb{R}^{L \\times L}, W^{out} \\in \\mathbb{R}^{K \\times L}$ are weight matrices charecterizing the connections between neurons in the layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "At time $n = 0$ the recurrent network state $\\mathbf{x}(0)$ is often set to the zero vector $\\mathbf{x}(0) = \\mathbf{0}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weight_matrix(in_dim, out_dim, key, scale=1e-2):\n",
    "    w = jax.random.normal(key, (out_dim, in_dim))\n",
    "    return scale*w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_bias(dim, key, scale=1e-2):\n",
    "    b = jax.random.normal(key, (dim, ))\n",
    "    return scale*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state, params = (W^{in}, W, W^{out}, b)\n",
    "# sizes = (input dim, state dim, output dim.)\n",
    "def init_network(sizes, key):\n",
    "    keys = jax.random.split(key, len(sizes))\n",
    "    \n",
    "    Win = init_weight_matrix(sizes[0], sizes[1], keys[0])\n",
    "    W = init_weight_matrix(sizes[1], sizes[1], keys[1])\n",
    "    Wout = init_weight_matrix(sizes[1], sizes[2], keys[3])\n",
    "    b = init_bias(sizes[1], keys[2])\n",
    "\n",
    "    return (Win, W, Wout, b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((25, 14), (25, 25), (14, 25), (25,))\n"
     ]
    }
   ],
   "source": [
    "K = len(S[0][0][0]) # K := input and output vector dim\n",
    "L = 25 # Reservoir or State Vector dim\n",
    "sizes = [K, L, K]\n",
    "params = init_network(sizes, key)\n",
    "print(jax.tree_map(lambda x: x.shape, params)) # printing shape of network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_og is initially the zero vector\n",
    "def forward_bp(params, u, x_og=np.zeros((L, ))):\n",
    "    \"\"\" Loop over the time steps of the input sequence\n",
    "    u[n] := [u_0, ..., u_{n_max}] where u_i \\in [0, 1]^K or (K, )\n",
    "    x_og: \\in R^L or (L, )\n",
    "    \"\"\"\n",
    "    Win, W, Wout, b = params\n",
    "    x = x_og.copy()\n",
    "\n",
    "    def apply_fun_scan(params, x, ut):\n",
    "        \"\"\" Perform single step update of the network.\n",
    "        x:  (L, )\n",
    "        un: (K, )\n",
    "        \"\"\"\n",
    "        Win, W, Wout, b = params\n",
    "        x = np.tanh(\n",
    "            np.dot(Win, ut) + np.dot(W, x) + b\n",
    "        )\n",
    "        y = jax.nn.sigmoid(np.dot(Wout, x))\n",
    "        return x, y     # this returns nan at some point according to nan debugger\n",
    "\n",
    "    f = functools.partial(apply_fun_scan, params)\n",
    "    _, Y = jax.lax.scan(f, x, u)\n",
    "    return Y\n",
    "\n",
    "batch_forward_bp = jax.vmap(forward_bp, in_axes=(None, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function\n",
    "Logistic regression is used for the case where you do not model loudness. Because there it is a categorical task (to hit or not to hit). But if you do model loudness you use the procedure linear regression. Meaning using the loss functions that are mentioned in the RNN section of the reader like MSE or quadratic loss.\n",
    "\n",
    "Time series prediction task $S = (\\mathbf{u}^{(i)}(n), \\mathbf{y}^{(i)}(n))_{i=1, ..., N;n=1, ..., n_i}$ where $\\mathbf{y}^{i}(n) = \\mathbf{u}^{(i)}(n+1)$\n",
    "For now: quadratic loss which is used in stationary tasks\n",
    "$$\n",
    "    L(\\hat{\\mathbf{Y}}_{i, \\theta}^{\\text{train}}, \\mathbf{Y}_i^{\\text{train}}) = \\parallel \\hat{\\mathbf{Y}}_{i, \\theta}^{\\text{train}} - \\mathbf{Y}_i^{\\text{train}} \\parallel^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization\n",
    "$$\n",
    "\\text{reg}(\\theta) = \\sum_{w \\in \\theta} w^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# could be made nicer\n",
    "def getParameterVector(params):\n",
    "    theta = []\n",
    "    for w in params:\n",
    "        for e in w:\n",
    "            if (e.size > 1):\n",
    "                for i in e:\n",
    "                    theta.append(i)\n",
    "            else:\n",
    "                theta.append(e)\n",
    "    return np.array(theta)\n",
    "\n",
    "def reg(params):\n",
    "    theta = getParameterVector(params)\n",
    "    return np.sum(np.square(theta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathcal{R}^{\\text{emp}}(\\theta) = \\frac{1}{N} \\sum^{N}_{i=1} L(\\hat{\\mathbf{Y}}_{i, \\theta}^{\\text{train}}, \\mathbf{Y}_i^{\\text{train}}) + r^2 \\; \\text{reg}(\\theta)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(params, u, y_true, alpha):\n",
    "    y_hat = batch_forward_bp(params, u)\n",
    "    return np.square(np.linalg.norm(np.subtract(y_hat, y_true))) + (alpha*alpha)*reg(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "    \\theta^{(n+1)} = \\theta^{(n)} - \\mu \\nabla \\mathcal{R}^{\\text{emp}}(\\theta^{(n)}),\n",
    "$$\n",
    "\n",
    "$$\n",
    "   \\nabla \\mathcal{R}^{\\text{emp}}(\\theta^{(n)}) = \n",
    "\\bigg(\\frac{\\partial  R^{emp}}{\\partial  w_1}(\\theta^{(n)}), ...,\\frac{\\partial  R^{emp}}{\\partial w_D}(\\theta^{(n)}) \\bigg)',\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Error detected: for reasonably sized data sets after the second epoch\n",
    "the parameter vector theta = {W_in, W, W_out, b} contains nan values\n",
    "(why ?????)\n",
    "presumably: gradient is nan or is it????\n",
    "(why gradient nan -> ?????????)\n",
    "\"\"\"\n",
    "@jax.jit\n",
    "def update(params, u, y_true, alpha, step_size=1e-2):\n",
    "\n",
    "    grads = jax.grad(loss)(params, u, y_true, alpha)\n",
    "    \"\"\"\n",
    "    print(len(grads))\n",
    "    print(grads[0].shape)\n",
    "    print(grads[1].shape)\n",
    "    print(grads[2].shape)\n",
    "    print(grads[3].shape)\n",
    "    print(len(params))\n",
    "    print(params[0].shape)\n",
    "    print(params[1].shape)\n",
    "    print(params[2].shape)\n",
    "    print(params[3].shape)\n",
    "    \"\"\"\n",
    "    return [\n",
    "        (w - step_size * dw)\n",
    "        for w, dw in zip(params, grads)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nanCheck(X):\n",
    "    print(np.isnan(np.sum(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "    \\mathcal{A}(S) = \\theta_{\\text{opt}} = \\underset{\\theta \\in \\Theta}{\\text{argmin}} \\; \\underbrace{\\frac{1}{N} \\sum^N_{i=1} L(\\hat{\\mathbf{Y}}_{\\theta}^{\\text{train}}, \\mathbf{Y}^{\\text{train}})}_{\\mathcal{R}^{\\text{emp}}(\\theta)},\n",
    "$$\n",
    "$$\n",
    "    \\theta^{(n+1)} = \\theta^{(n)} - \\mu \\nabla \\mathcal{R}^{\\text{emp}}(\\theta^{(n)}),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "(2430, 14)\n",
      "(6, 2430, 14)\n",
      "(6, 2430, 14)\n",
      "(6, 2430, 14)\n",
      "(6, 2430, 14)\n",
      "Epoch  1 (26.48s): train loss 13089.08 test loss 13086.56| Epoch  2 (22.47s): train loss 55.29 test loss 49.42| Epoch  3 (6.48s): train loss 55.29 test loss 49.42| Epoch  4 (5.01s): train loss 55.29 test loss 49.42| Epoch  5 (3.91s): train loss 55.29 test loss 49.42| Epoch  6 (3.96s): train loss 55.29 test loss 49.42| Epoch  7 (3.70s): train loss 55.29 test loss 49.42| Epoch  8 (4.26s): train loss 55.29 test loss 49.42| Epoch  9 (6.40s): train loss 55.29 test loss 49.42| Epoch 10 (6.62s): train loss 55.29 test loss 49.42| (6, 2430, 14)\n",
      "(6, 2430, 14)\n",
      "(6, 2430, 14)\n",
      "(6, 2430, 14)\n",
      "Epoch  1 (6.44s): train loss 49.42 test loss 55.29| Epoch  2 (6.53s): train loss 49.42 test loss 55.29| Epoch  3 (6.42s): train loss 49.42 test loss 55.29| Epoch  4 (6.39s): train loss 49.42 test loss 55.29| Epoch  5 (6.14s): train loss 49.42 test loss 55.29| Epoch  6 (6.33s): train loss 49.42 test loss 55.29| Epoch  7 (6.28s): train loss 49.42 test loss 55.29| Epoch  8 (6.33s): train loss 49.42 test loss 55.29| Epoch  9 (6.53s): train loss 49.42 test loss 55.29| Epoch 10 (6.44s): train loss 49.42 test loss 55.29| (6, 2430, 14)\n",
      "(6, 2430, 14)\n",
      "(6, 2430, 14)\n",
      "(6, 2430, 14)\n",
      "Epoch  1 (6.16s): train loss 385459.41 test loss 385453.53| Epoch  2 (6.44s): train loss 370197.41 test loss 370191.53| Epoch  3 (6.50s): train loss 355539.78 test loss 355533.91| Epoch  4 (6.41s): train loss 341462.59 test loss 341456.72| Epoch  5 (6.22s): train loss 327942.84 test loss 327936.97| Epoch  6 (6.27s): train loss 314958.53 test loss 314952.66| Epoch  7 (6.08s): train loss 302488.34 test loss 302482.47| Epoch  8 (6.11s): train loss 290511.97 test loss 290506.09| Epoch  9 (6.52s): train loss 279009.91 test loss 279004.03| Epoch 10 (6.49s): train loss 267963.28 test loss 267957.41| (6, 2430, 14)\n",
      "(6, 2430, 14)\n",
      "(6, 2430, 14)\n",
      "(6, 2430, 14)\n",
      "Epoch  1 (5.94s): train loss 257348.28 test loss 257354.16| Epoch  2 (5.75s): train loss 247159.25 test loss 247165.12| Epoch  3 (5.77s): train loss 237373.70 test loss 237379.58| Epoch  4 (5.92s): train loss 227975.67 test loss 227981.55| Epoch  5 (5.99s): train loss 218949.78 test loss 218955.66| Epoch  6 (6.14s): train loss 210281.33 test loss 210287.20| Epoch  7 (6.61s): train loss 201956.14 test loss 201962.02| Epoch  8 (6.43s): train loss 193960.64 test loss 193966.52| Epoch  9 (6.61s): train loss 186281.77 test loss 186287.64| Epoch 10 (6.75s): train loss 178906.95 test loss 178912.83| Epoch  1 (20.65s): train loss 55.29 |\n",
      "Epoch  2 (3.84s): train loss 55.29 |\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "s is a list of tuples \n",
    "[(u1, y1), (u2, y2), ...] -> ((u_1, u_2, ...), (y_1, y_2, ...))\n",
    "a signal u_i or y_i is (K, n_i) where n_i is the length of the song which can differ\n",
    "and K is the number of modeled drum parts\n",
    "\"\"\"\n",
    "def unpack(s):\n",
    "    f1 = map(lambda x: x[0], s)\n",
    "    f2 = map(lambda x: x[1], s)\n",
    "\n",
    "    u_batch = np.array(list(f1))\n",
    "    y_batch = np.array(list(f2))\n",
    "    #nanCheck(u_batch)\n",
    "    #nanCheck(y_batch)\n",
    "    # try and remove nan values here?\n",
    "    #u_batch = u_batch[np.logical_not(np.isnan(u_batch))]\n",
    "    #y_batch = y_batch[np.logical_not(np.isnan(y_batch))]\n",
    "    return (u_batch, y_batch)\n",
    "\n",
    "\"\"\"\n",
    "trains the network on the training data for n epochs\n",
    "in the cross validations setup the testing data is\n",
    "the validation set and is simply used to measure the\n",
    "testing loss next to the training loss\n",
    "\"\"\"\n",
    "def train(params, u_train, y_train, u_test, y_test, alpha, n_epochs=10):\n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "    for epoch in range(n_epochs):\n",
    "        start_time = time.time()\n",
    "\n",
    "        params = update(params, u_train, y_train, alpha)\n",
    "        train_loss.append(loss(params, u_train, y_train, alpha))\n",
    "        test_loss.append(loss(params, u_test, y_test, alpha))\n",
    "\n",
    "        epoch_time = time.time() - start_time\n",
    "\n",
    "        print(f'Epoch {epoch+1:>2} ({epoch_time:<.2f}s): ', end='')\n",
    "        print(f'train loss {train_loss[-1]:<5.2f} test loss {test_loss[-1]:<5.2f}', end='| ')\n",
    "    \n",
    "    return params\n",
    "\n",
    "\"\"\"\n",
    "same as train but without testing set to measure testing loss\n",
    "\"\"\"\n",
    "def train2(params, u_train, y_train, r, n_epochs=2):\n",
    "    train_loss = []\n",
    "    for epoch in range(n_epochs):\n",
    "        start_time = time.time()\n",
    "\n",
    "        params = update(params, u_train, y_train, r)\n",
    "        train_loss.append(loss(params, u_train, y_train, r))\n",
    "\n",
    "        epoch_time = time.time() - start_time\n",
    "\n",
    "        print(f'Epoch {epoch+1:>2} ({epoch_time:<.2f}s): ', end='')\n",
    "        print(f'train loss {train_loss[-1]:<5.2f} |')\n",
    "    return params\n",
    "\n",
    "if (TRAINING_ON and not RC_ON):\n",
    "    # given S, params, loss/emprical risk, r (regularization)\n",
    "    # Hyperparameters\n",
    "    print(len(S))\n",
    "    print(S[0][0].shape)\n",
    "    step_size=1e-2\n",
    "    k = 2   # k fold cross validation\n",
    "    # r is the hyperparameter (here r = alpha for the regularization)\n",
    "    validation_risk_r = []\n",
    "\n",
    "    # split S into k disjoint subsets\n",
    "\n",
    "    \"\"\"\n",
    "    Edge case: if k > amount of tuples (u_train, y_train) in S then n = 0\n",
    "    which will result in a valueError when constructing S_k.\n",
    "    Furthermore: the procedure will break down for single (u_train, y_train)\n",
    "    because then the reduced training set will be the empty set\n",
    "    \"\"\"\n",
    "    n = (int) (len(S)/k)\n",
    "\n",
    "    S_k = [S[i : i + n] for i in range(0, len(S), n)]\n",
    "\n",
    "    for r in range(0, 2):\n",
    "        validation_risk = []\n",
    "        for j in range(0, k):\n",
    "            V = S_k.pop(j)                          # validation set\n",
    "            T = [x for l in S_k for x in l]         # reduced training set\n",
    "            S_k.insert(j, V)\n",
    "            u_train, y_train = unpack(T)\n",
    "            print(u_train.shape)\n",
    "            print(y_train.shape)\n",
    "            u_val, y_val = unpack(V)\n",
    "            print(u_val.shape)\n",
    "            print(y_val.shape)\n",
    "            params = train(params, u_train, y_train, u_val, y_val, r)\n",
    "            validation_risk.append(loss(params, u_val, y_val, r))\n",
    "        validation_risk_r.append(np.mean(np.array(validation_risk)))\n",
    "\n",
    "    r_opt = np.argmin(np.array(validation_risk_r))\n",
    "    u_train, y_train = unpack(V)\n",
    "    params = train2(params, u_train, y_train, r_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done writing list into a binary file\n"
     ]
    }
   ],
   "source": [
    "# https://pynative.com/python-write-list-to-file/\n",
    "# write list to binary file\n",
    "def write_list2(a_list):\n",
    "    # store list in binary file so 'wb' mode\n",
    "    with open('params', 'wb') as fp:\n",
    "        pickle.dump(a_list, fp)\n",
    "        print('Done writing list into a binary file')\n",
    "\n",
    "# Read list to memory\n",
    "def read_list2():\n",
    "    # for reading also binary mode is important\n",
    "    with open('params', 'rb') as fp:\n",
    "        n_list = pickle.load(fp)\n",
    "        return n_list\n",
    "\n",
    "\n",
    "if (READ_PARAMETERS):\n",
    "    params = tuple(read_list2())\n",
    "elif (SAVE_PARAMETERS):\n",
    "    write_list2(params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Music Generation\n",
    "Get the file to prime the network with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "pop from empty list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Matth\\OneDrive\\Documenten\\NN\\semester-project\\networks\\RNNtest.ipynb Cell 47'\u001b[0m in \u001b[0;36m<cell line: 34>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Matth/OneDrive/Documenten/NN/semester-project/networks/RNNtest.ipynb#ch0000043?line=30'>31</a>\u001b[0m     x, y \u001b[39m=\u001b[39m apply_fun_scan(params, x, y)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Matth/OneDrive/Documenten/NN/semester-project/networks/RNNtest.ipynb#ch0000043?line=31'>32</a>\u001b[0m     y_signal\u001b[39m.\u001b[39mappend(y)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Matth/OneDrive/Documenten/NN/semester-project/networks/RNNtest.ipynb#ch0000043?line=33'>34</a>\u001b[0m dfo \u001b[39m=\u001b[39m D_inv(np\u001b[39m.\u001b[39;49marray(y_signal), h_inv)\n",
      "\u001b[1;32mc:\\Users\\Matth\\OneDrive\\Documenten\\NN\\semester-project\\networks\\RNNtest.ipynb Cell 7'\u001b[0m in \u001b[0;36mD_inv\u001b[1;34m(y, h_inv)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Matth/OneDrive/Documenten/NN/semester-project/networks/RNNtest.ipynb#ch0000006?line=31'>32</a>\u001b[0m dfo[\u001b[39m\"\u001b[39m\u001b[39mvelocity\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m velocity_new\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Matth/OneDrive/Documenten/NN/semester-project/networks/RNNtest.ipynb#ch0000006?line=32'>33</a>\u001b[0m dfo[\u001b[39m\"\u001b[39m\u001b[39mnotes\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m notes_new\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Matth/OneDrive/Documenten/NN/semester-project/networks/RNNtest.ipynb#ch0000006?line=34'>35</a>\u001b[0m latestTime \u001b[39m=\u001b[39m time_new\u001b[39m.\u001b[39;49mpop() \u001b[39m+\u001b[39m \u001b[39m10\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Matth/OneDrive/Documenten/NN/semester-project/networks/RNNtest.ipynb#ch0000006?line=36'>37</a>\u001b[0m pre \u001b[39m=\u001b[39m [[\u001b[39m0\u001b[39m,\u001b[39m0\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mHeader\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m480\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Matth/OneDrive/Documenten/NN/semester-project/networks/RNNtest.ipynb#ch0000006?line=37'>38</a>\u001b[0m         [\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mStart_track\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Matth/OneDrive/Documenten/NN/semester-project/networks/RNNtest.ipynb#ch0000006?line=38'>39</a>\u001b[0m         [\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mTime_signature\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m4\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m24\u001b[39m, \u001b[39m8\u001b[39m],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Matth/OneDrive/Documenten/NN/semester-project/networks/RNNtest.ipynb#ch0000006?line=39'>40</a>\u001b[0m         [\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mTitle_t\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39mfrom model\u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39m\"\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Matth/OneDrive/Documenten/NN/semester-project/networks/RNNtest.ipynb#ch0000006?line=40'>41</a>\u001b[0m         [\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mEnd_track\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Matth/OneDrive/Documenten/NN/semester-project/networks/RNNtest.ipynb#ch0000006?line=41'>42</a>\u001b[0m         [\u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mStart_track\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m]]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Matth/OneDrive/Documenten/NN/semester-project/networks/RNNtest.ipynb#ch0000006?line=43'>44</a>\u001b[0m post \u001b[39m=\u001b[39m [[\u001b[39m2\u001b[39m, latestTime, \u001b[39m\"\u001b[39m\u001b[39mEnd_track\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Matth/OneDrive/Documenten/NN/semester-project/networks/RNNtest.ipynb#ch0000006?line=44'>45</a>\u001b[0m         [\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mEnd_of_file\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m]]\n",
      "\u001b[1;31mIndexError\u001b[0m: pop from empty list"
     ]
    }
   ],
   "source": [
    "def apply_fun_scan(params, x, un):\n",
    "    \"\"\" Perform single step update of the network.\n",
    "    x:  (L, ) at time step n -> x: (L, ) at time step n+1\n",
    "    un: (K, )\n",
    "    \"\"\"\n",
    "    Win, W, Wout, b = params\n",
    "    x = np.tanh(\n",
    "        np.dot(Win, un) + np.dot(W, x) + b\n",
    "    )\n",
    "    y = jax.nn.sigmoid(np.dot(Wout, x))\n",
    "    return x, y\n",
    "\"\"\"\n",
    "columnNames = [\"track\", \"time\", \"control\"\n",
    "                                , \"channel\", \"notes\", \"velocity\"]\n",
    "\n",
    "df1 = pd.read_csv('drumDemo.csv', skipfooter=2, skiprows=8, engine='python', names= columnNames)\n",
    "\n",
    "u_prime = D(df1, h, maxTime)[0]\n",
    "\"\"\"\n",
    "u_prime = S[0][0]\n",
    "n_stop = 50\n",
    "n_output = 4000\n",
    "\n",
    "y_signal = []\n",
    "x = np.zeros((L,))\n",
    "for n in range(n_stop):\n",
    "    x, y = apply_fun_scan(params, x, u_prime[n])\n",
    "    y_signal.append(y)\n",
    "\n",
    "for n in range(n_output):\n",
    "    x, y = apply_fun_scan(params, x, y)\n",
    "    y_signal.append(y)\n",
    "\n",
    "dfo = D_inv(np.array(y_signal), h_inv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathcal{M}^{-1} : \\text{CSV} \\rightarrow \\text{MIDI}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Puts generated CSV file in correct folder\n",
    "and generates the MIDI file and puts that in the correct\n",
    "folder as well.\n",
    "\"\"\"\n",
    "# for debugging: checking highest amplitude in output signal\n",
    "\"\"\"\n",
    "v = -99999999999999\n",
    "for e in y_signal:\n",
    "    if e.max() >= v:\n",
    "        v = e.max()\n",
    "print(v)\n",
    "\"\"\"\n",
    "\n",
    "os.chdir('../midi/NetworkOutputCSV')\n",
    "name = \"rnn.csv\"\n",
    "dfo.to_csv(name, index = False, header = False)\n",
    "copy(name, '../midicsv-1.1')\n",
    "os.chdir('../midicsv-1.1')\n",
    "\n",
    "command = \"csvmidi\" + \" \" + name + \" \" + name[:-4] + \".mid\"\n",
    "res = os.system(command)\n",
    "print(res)\n",
    "os.remove(name)\n",
    "try:\n",
    "    move(name[:-4] + \".mid\", '../NetworkOutputMIDI')\n",
    "except:\n",
    "    print(\"midi file already here\")\n",
    "    os.remove(name[:-4] + \".mid\")\n",
    "\n",
    "os.chdir(cwd)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental: reservoir computing\n",
    "dimensionality $K, L, K$ same: using same rnn.\n",
    "First: scaling with spectral radius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (RC_ON):\n",
    "    W, Win, Wout, b = params\n",
    "    rhoW = np.max(np.absolute(np.linalg.eig(W)[0]))\n",
    "    scale = 1.25\n",
    "    W = (scale/rhoW)*W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: state harvesting: record activation for each of the $L$ reservoir neurons\n",
    "when driven by teacher input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(u, Win, W, b, Wout=None, x_init=np.zeros((L, ))):\n",
    "    # u: (n_max, K)\n",
    "    n_max = u.shape[0]\n",
    "    K = u.shape[1]\n",
    "    X, Y = [], []\n",
    "    # X: (1+K+L, n_max)\n",
    "    # Y: (K, n_max) double chheck later\n",
    "    x = x_init.copy()\n",
    "    for t in range(u.shape[0]):\n",
    "        x = np.tanh(\n",
    "            np.dot(Win, u[t]) + np.dot(W, x) + b\n",
    "        )\n",
    "        full_state = np.concatenate(u[t], x, b)\n",
    "        X.append(full_state)\n",
    "        if Wout is not None:\n",
    "            y = np.dot(Wout, full_state)\n",
    "            Y.append(y)\n",
    "        # when returning: need to transpose data arrays, such that dim: (n_max, x)\n",
    "    if Wout is None:\n",
    "        return x, np.array(X).T\n",
    "    else:\n",
    "        return x, np.array(X).T, np.array(Y).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (RC_ON):\n",
    "    x, X = forward(u_train[0], Win, W)\n",
    "    print(f'u: {u_train[0].shape}, x: {x.shape}, X: {X.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 2: compute readouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (RC_ON):\n",
    "    reg = 1e-8\n",
    "\n",
    "    Wout_rc_ = np.dot(\n",
    "        np.dot(y_train[0].T, X.T),\n",
    "        np.linalg.inv(\n",
    "            np.dot(X, X.T) + reg*np.eye(1+K+L)\n",
    "        )\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e375b77a672b76be28c1e80386a6d4db61c866c142c6acff7bc94a65b4573147"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

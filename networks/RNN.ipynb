{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN in Jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as np\n",
    "import math\n",
    "import numpy as onp\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from jax import grad, vmap, jit\n",
    "import jax\n",
    "import jax.numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import functools\n",
    "import os\n",
    "from shutil import copy\n",
    "from shutil import move\n",
    "import pickle\n",
    "from jax.config import config\n",
    "config.update(\"jax_debug_nans\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "TRAINING_ON = False\n",
    "PROCESSING_AND_DATA_LOADING_ON = False\n",
    "SAVE_PARAMETERS = False\n",
    "READ_PARAMETERS = True\n",
    "RC_ON = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre - and Postprocessing\n",
    "$$\n",
    "  \\mathcal{D} : \\mathbf{CSV}_{\\text{drum}} \\rightarrow [0,1]^{K \\times n_i}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Given a dataframe of a drum track and a hashmap\n",
    "for mapping the drum kits to an index of the vector.\n",
    "Returns the vectorized training set\n",
    "consisting of the training input and output.\n",
    "maxTime is for zero padding. If a signal is shorter than maxTime\n",
    "then it is zero padded.\n",
    "D is adjusted to also return the training output.\n",
    "\"\"\"\n",
    "def D(df, h, maxTime):\n",
    "\n",
    "    time = df[\"time\"].values\n",
    "    notes = df[\"notes\"].values\n",
    "    velocity = df[\"velocity\"].values\n",
    "\n",
    "    dim = len(h)\n",
    "    u = list(itertools.repeat(np.zeros(dim), max(time)+1))\n",
    "\n",
    "    for i in range(len(notes)):\n",
    "        v = [0]*dim\n",
    "        v[h[int(notes[i])]] = int(velocity[i]) / 127\n",
    "        if (np.any(u[time[i]])):        #If the current vector is not the zero vector\n",
    "            u[time[i]] = u[time[i]] + np.array(v)\n",
    "        else:\n",
    "            u[time[i]] = np.array(v)\n",
    "    \n",
    "    y_train = u[1:]\n",
    "    y_train.append(np.zeros(dim))\n",
    "\n",
    "    padding = maxTime - max(time)\n",
    "    return np.pad(np.array(u), ((0, padding), (0, 0)), \"constant\"), np.pad(np.array(y_train), ((0, padding), (0, 0)), \"constant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "  \\mathcal{D}^{-1} : [0,1]^{K \\times n_i} \\rightarrow \\mathbf{CSV}_{\\text{drum}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Given an output vector and a hashmap\n",
    "for mapping indices of the vector to the\n",
    "appropriate note/drum kit. Returns a dataframe\n",
    "that can be converted to a csv file and then to a midi file\n",
    "\"\"\"\n",
    "def D_inv(y, h_inv):\n",
    "    dfo = pd.DataFrame([], columns= [\"track\", \"time\", \"control\"\n",
    "                            ,\"channel\", \"notes\", \"velocity\", \"filler\"])\n",
    "    time_new = []\n",
    "    notes_new = []\n",
    "    velocity_new = []\n",
    "\n",
    "    track_new = []\n",
    "    control_new = []\n",
    "    channel_new = []\n",
    "\n",
    "    for n in range(0, len(y), 30):\n",
    "        if (np.any(y[n])):\n",
    "            yn = y[n]\n",
    "            for i in range(len(yn)):\n",
    "                v = (int)(yn[i] * 127)\n",
    "                track_new.append(2)\n",
    "                time_new.append(n)\n",
    "                channel_new.append(9)\n",
    "                notes_new.append(h_inv[i])\n",
    "                velocity_new.append(v)\n",
    "                if (v > 0):\n",
    "                    control_new.append(\"Note_on_c\")\n",
    "                else:\n",
    "                    control_new.append(\"Note_off_c\")\n",
    "\n",
    "    dfo[\"track\"] = track_new\n",
    "    dfo[\"time\"] = time_new\n",
    "    dfo[\"control\"] = control_new\n",
    "    dfo[\"channel\"] = channel_new\n",
    "    dfo[\"notes\"] = notes_new\n",
    "    dfo[\"velocity\"] = velocity_new\n",
    "\n",
    "\n",
    "    latestTime = time_new[-1] + 10\n",
    "\n",
    "    pre = [[0,0, \"Header\", 1, 2, 480, ''],\n",
    "            [1, 0, \"Start_track\", '', '', '', ''],\n",
    "            [1, 0, \"Time_signature\", 4, 2, 24, 8],\n",
    "            [1, 0, \"Title_t\", \"\\\"from model\\\"\", '', '', ''],\n",
    "            [1, 0, \"End_track\", '', '', '', ''],\n",
    "            [2, 0, \"Start_track\", '', '', '', '']]\n",
    "    dfo_pre = pd.DataFrame(pre, columns =[\"track\", \"time\", \"control\"\n",
    "                            ,\"channel\", \"notes\", \"velocity\", \"filler\"])\n",
    "\n",
    "    post = [[2, latestTime, \"End_track\", '', '', '', ''],\n",
    "            [0, 0, \"End_of_file\", '', '', '', '']]\n",
    "\n",
    "    dfo_post = pd.DataFrame(post, columns =[\"track\", \"time\", \"control\"\n",
    "                            ,\"channel\", \"notes\", \"velocity\", \"filler\"])\n",
    "\n",
    "    # adding pre and post\n",
    "    dfo = pd.concat([dfo_pre, dfo, dfo_post])\n",
    "\n",
    "    return dfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "    \\mathcal{T} : \\mathbf{CSV} \\rightarrow \\mathbf{CSV}^J_{\\text{drum}}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Returns a list of dataframes for each track\n",
    "seperated by \"start track\" and \"end track\"\n",
    "\"\"\"\n",
    "def getTracks(df):\n",
    "    subDataFrames = []\n",
    "    startingIdx = -1\n",
    "    # get each track as seperate dataframe\n",
    "    for idx, row in df.iterrows():\n",
    "        if (row[\"control\"] == \" Start_track\"):\n",
    "            startingIdx = idx\n",
    "        elif (row[\"control\"] == \" End_track\"):\n",
    "            subDataFrames.append(df[startingIdx : idx + 1])\n",
    "    return subDataFrames\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Given a dataFrame converted from a MIDI file\n",
    "returns a list of dataFrames of just the channel 9 tracks\n",
    "and removes all rows where the control column is\n",
    "not \"note_on_c\"\n",
    "i.e. the drum tracks\n",
    "Meaning that if a song has multiple drum tracks the list\n",
    "will contain more than one dataframe.\n",
    "\"\"\"\n",
    "def getDrumTracks(df):\n",
    "    # get those tracks that are drum tracks\n",
    "    drumDataFrames = [d[d[\"control\"] == \" Note_on_c\"] for d in getTracks(df) if \" 9\" in list(d[\"channel\"])]\n",
    "    return drumDataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "    \\mathcal{M} : \\text{MIDI} \\rightarrow \\text{CSV}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv file already there\n",
      "csv file already there\n",
      "csv file already there\n",
      "csv file already there\n",
      "csv file already there\n",
      "csv file already there\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "Copies over the MIDI files to the folder \n",
    "where the can be converted. Once converted the CSV\n",
    "files are moved to the NetworkInput CSV folder.\n",
    "\n",
    "\"\"\"\n",
    "cwd = os.getcwd()\n",
    "\n",
    "os.chdir('../midi/NetworkInputMIDI')\n",
    "\n",
    "fileNames = []\n",
    "for m in os.listdir():\n",
    "    fileNames.append(m)\n",
    "    copy(m, '../midicsv-1.1')\n",
    "\n",
    "os.chdir('../midicsv-1.1')\n",
    "for m in fileNames:\n",
    "    command = \"midicsv\" + \" \" + m + \" \" + m[:-4] + \".csv\"\n",
    "    res = os.system(command)\n",
    "    os.remove(m)\n",
    "    try:\n",
    "        move(m[:-4] + \".csv\", '../NetworkInputCSV')\n",
    "    except:\n",
    "        print(\"csv file already there\")\n",
    "        os.remove(m[:-4] + \".csv\")\n",
    "\n",
    "# ensures that the cwd resets\n",
    "os.chdir(cwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now import the csv files as panda dataframes (get just the drum tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../midi/NetworkInputCSV')\n",
    "\n",
    "columnNames = [\"track\", \"time\", \"control\"\n",
    "                                , \"channel\", \"notes\", \"velocity\"]\n",
    "\n",
    "\"\"\"\n",
    "Loop through all the csv files in\n",
    "the network input folder\n",
    "\"\"\"\n",
    "drumTracks_df = []\n",
    "for csv in os.listdir():\n",
    "    # for now: generalize later\n",
    "    df = pd.read_csv(csv, skiprows=6, engine='python', names= columnNames)\n",
    "    df.dropna()\n",
    "    drumTracks_df = drumTracks_df + getDrumTracks(df)\n",
    "\n",
    "os.chdir(cwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create hashmap that maps notes to right vector index and additionally\n",
    "1. normalize time\n",
    "2. get max time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{35: 0, 36: 1, 37: 2, 38: 3, 39: 4, 40: 5, 41: 6, 42: 7, 43: 8, 44: 9, 45: 10, 46: 11, 47: 12, 48: 13, 49: 14, 50: 15, 51: 16, 56: 17, 57: 18, 59: 19, 60: 20, 61: 21, 63: 22, 68: 23, 69: 24, 70: 25, 76: 26, 80: 27, 81: 28, 82: 29, 87: 30, 88: 31, 89: 32, 91: 33, 93: 34}\n",
      "{0: 35, 1: 36, 2: 37, 3: 38, 4: 39, 5: 40, 6: 41, 7: 42, 8: 43, 9: 44, 10: 45, 11: 46, 12: 47, 13: 48, 14: 49, 15: 50, 16: 51, 17: 56, 18: 57, 19: 59, 20: 60, 21: 61, 22: 63, 23: 68, 24: 69, 25: 70, 26: 76, 27: 80, 28: 81, 29: 82, 30: 87, 31: 88, 32: 89, 33: 91, 34: 93}\n"
     ]
    }
   ],
   "source": [
    "h = {}\n",
    "h_inv = {}\n",
    "unique_notes = set()\n",
    "\n",
    "normalizingFactorFound = False\n",
    "for d in drumTracks_df:\n",
    "    d[\"notes\"] = d[\"notes\"].transform(lambda x : int(x))\n",
    "    unique_notes = unique_notes.union(set(d[\"notes\"]))\n",
    "    normTime = d.iloc[0][\"time\"]\n",
    "    d[\"time\"] = d[\"time\"].transform(lambda x : x - normTime)\n",
    "\n",
    "notes = list(unique_notes)\n",
    "notes.sort()\n",
    "\n",
    "for idx in range(0, len(notes)):\n",
    "    h[notes[idx]] = idx\n",
    "    h_inv[idx] = notes[idx]\n",
    "\n",
    "print(h)\n",
    "print(h_inv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seperate drum tracks into bars every 2222.222 ms is a bar\n",
    "$$\n",
    "    \\mathcal{B} : \\mathbf{CSV}^J_{\\text{drum}} \\rightarrow \\mathbf{CSV}^B_{\\text{drum}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "744\n"
     ]
    }
   ],
   "source": [
    "def seperateIntoBars(df, barLength=2222):\n",
    "    subDataFrames = []\n",
    "    startingTime = 0\n",
    "    startingIdx = 0\n",
    "    idx = 0\n",
    "    for _, row in df.iterrows():\n",
    "        if (row[\"time\"] - startingTime > barLength and (row[\"time\"] - startingTime < barLength + 1000)) :\n",
    "            subDataFrames.append(df[startingIdx : idx + 1].copy())\n",
    "            startingTime = row[\"time\"]\n",
    "            startingIdx = idx\n",
    "        idx = idx + 1\n",
    "    \n",
    "    for d in subDataFrames:\n",
    "        normTime = d.iloc[0][\"time\"]\n",
    "        d[\"time\"] = d[\"time\"].transform(lambda x : x - normTime)\n",
    "    return subDataFrames\n",
    "\n",
    "df_bars = []\n",
    "for df in drumTracks_df:\n",
    "    df_bars = df_bars + seperateIntoBars(df)\n",
    "\n",
    "maxTime = -1\n",
    "for d in df_bars:\n",
    "    for idx, row in d.iterrows():\n",
    "        if (row[\"time\"] > maxTime):\n",
    "            maxTime = row[\"time\"]\n",
    "\n",
    "print(len(drumTracks_df))\n",
    "print(len(df_bars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get vectors from drum track dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (PROCESSING_AND_DATA_LOADING_ON):\n",
    "    S = []  # S will be a list of tuples\n",
    "            # each tuple contains two input signals over time\n",
    "\n",
    "    for df in df_bars:\n",
    "        u_train, y_train = D(df, h, maxTime)\n",
    "        S.append((u_train, y_train))\n",
    "\n",
    "    u_ex, y_ex = S[0]\n",
    "    print(jax.tree_map(lambda x: x.shape, u_ex))\n",
    "    print(jax.tree_map(lambda x: x.shape, y_ex))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save vectors so we do not have to rerun it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pynative.com/python-write-list-to-file/\n",
    "# write list to binary file\n",
    "def write_list(a_list, name):\n",
    "    # store list in binary file so 'wb' mode\n",
    "    with open(name, 'wb') as fp:\n",
    "        pickle.dump(a_list, fp)\n",
    "        print('Done writing list into a binary file')\n",
    "\n",
    "# Read list to memory\n",
    "def read_list(name):\n",
    "    # for reading also binary mode is important\n",
    "    with open(name, 'rb') as fp:\n",
    "        n_list = pickle.load(fp)\n",
    "        return n_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (PROCESSING_AND_DATA_LOADING_ON):\n",
    "    write_list(S, \"training\")\n",
    "else:\n",
    "    S = read_list(\"training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A RNN (with no LSTM units) is a neural network with recurrent connections (dynamical system)\n",
    "$$\n",
    "\\begin{array}{l l}\n",
    "          \\mathbf{x}(n+1) = \\sigma(W \\mathbf{x}(n) + W^{in}\\mathbf{u}(n+1) + \\mathbf{b}) \\\\\n",
    "          \\mathbf{y}(n) = f(W^{out} \\mathbf{x}(n)),\n",
    "\\end{array}\n",
    "$$\n",
    "Describes how the network activation state is updated and how output signal is generated. \n",
    "\n",
    "Input vector $\\mathbf{u}(n) \\in [0,1]^K$\n",
    "\n",
    "Activation/state vector $\\mathbf{x}(n) \\in \\mathbb{R}^L$\n",
    "\n",
    "Output vector $\\mathbf{y}(n) \\in \\mathbb{R}^K$\n",
    "\n",
    "Bias vector $\\mathbf{b} \\in \\mathbb{R}^L$\n",
    "\n",
    "$W^{in} \\in \\mathbb{R}^{L \\times K}, W \\in \\mathbb{R}^{L \\times L}, W^{out} \\in \\mathbb{R}^{K \\times L}$ are weight matrices charecterizing the connections between neurons in the layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "At time $n = 0$ the recurrent network state $\\mathbf{x}(0)$ is often set to the zero vector $\\mathbf{x}(0) = \\mathbf{0}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weight_matrix(in_dim, out_dim, key, scale=1e-2):\n",
    "    w = jax.random.normal(key, (out_dim, in_dim))\n",
    "    return scale*w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_bias(dim, key, scale=1e-2):\n",
    "    b = jax.random.normal(key, (dim, ))\n",
    "    return scale*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state, params = (W^{in}, W, W^{out}, b)\n",
    "# sizes = (input dim, state dim, output dim.)\n",
    "def init_network(sizes, key):\n",
    "    keys = jax.random.split(key, len(sizes))\n",
    "    \n",
    "    Win = init_weight_matrix(sizes[0], sizes[1], keys[0])\n",
    "    W = init_weight_matrix(sizes[1], sizes[1], keys[1])\n",
    "    Wout = init_weight_matrix(sizes[1], sizes[2], keys[3])\n",
    "    b = init_bias(sizes[1], keys[2])\n",
    "\n",
    "    return (Win, W, Wout, b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((1000, 35), (1000, 1000), (35, 1000), (1000,))\n"
     ]
    }
   ],
   "source": [
    "K = len(S[0][0][0]) # K := input and output vector dim\n",
    "L = 1000 # Reservoir or State Vector dim\n",
    "sizes = [K, L, K]\n",
    "params = init_network(sizes, key)\n",
    "print(jax.tree_map(lambda x: x.shape, params)) # printing shape of network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# derived from https://github.com/rug-minds/teaching/blob/main/boilerplate/reservoir.ipynb\n",
    "# x_og is initially the zero vector\n",
    "def forward_bp(params, u, x_og=np.zeros((L, ))):\n",
    "    \"\"\" Loop over the time steps of the input sequence\n",
    "    u[n] := [u_0, ..., u_{n_max}] where u_i \\in [0, 1]^K or (K, )\n",
    "    x_og: \\in R^L or (L, )\n",
    "    \"\"\"\n",
    "    Win, W, Wout, b = params\n",
    "    x = x_og.copy()\n",
    "\n",
    "    def apply_fun_scan(params, x, ut):\n",
    "        \"\"\" Perform single step update of the network.\n",
    "        x:  (L, )\n",
    "        un: (K, )\n",
    "        \"\"\"\n",
    "        Win, W, Wout, b = params\n",
    "        x = jax.nn.sigmoid(\n",
    "            np.dot(Win, ut) + np.dot(W, x) + b\n",
    "        )\n",
    "        y = jax.nn.sigmoid(np.dot(Wout, x))\n",
    "        return x, y     # this returns nan at some point according to nan debugger\n",
    "\n",
    "    f = functools.partial(apply_fun_scan, params)\n",
    "    _, Y = jax.lax.scan(f, x, u)\n",
    "    return Y\n",
    "\n",
    "batch_forward_bp = jax.vmap(forward_bp, in_axes=(None, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function\n",
    "Logistic regression is used for the case where you do not model loudness. Because there it is a categorical task (to hit or not to hit). But if you do model loudness you use the procedure linear regression. Meaning using the loss functions that are mentioned in the RNN section of the reader like MSE or quadratic loss.\n",
    "\n",
    "Time series prediction task $S = (\\mathbf{u}^{(i)}(n), \\mathbf{y}^{(i)}(n))_{i=1, ..., N;n=1, ..., n_i}$ where $\\mathbf{y}^{(i)}(n) = \\mathbf{u}^{(i)}(n+1)$.\n",
    "Quadratic loss which is used in stationary tasks\n",
    "$$\n",
    "    L(\\hat{\\mathbf{Y}}_{i, \\theta}^{\\text{train}}, \\mathbf{Y}_i^{\\text{train}}) = \\parallel \\hat{\\mathbf{Y}}_{i, \\theta}^{\\text{train}} - \\mathbf{Y}_i^{\\text{train}} \\parallel^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization\n",
    "$$\n",
    "\\text{reg}(\\theta) = \\sum_{w \\in \\theta} w^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg(params):\n",
    "    Win, W, Wout, b = params\n",
    "    return (\n",
    "        np.sum(np.square(Win)) +\n",
    "        np.sum(np.square(W)) +\n",
    "        np.sum(np.square(Wout)) +\n",
    "        np.sum(np.square(b))\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathcal{R}^{\\text{emp}}(\\theta) = \\frac{1}{N} \\sum^{N}_{i=1} L(\\hat{\\mathbf{Y}}_{i, \\theta}^{\\text{train}}, \\mathbf{Y}_i^{\\text{train}}) + r^2 \\; \\text{reg}(\\theta)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(params, u, y_true, alpha):\n",
    "    y_hat = batch_forward_bp(params, u)\n",
    "    return np.square(np.linalg.norm(np.subtract(y_hat, y_true))) + (alpha*alpha)*reg(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "    \\theta^{(n+1)} = \\theta^{(n)} - \\mu \\nabla \\mathcal{R}^{\\text{emp}}(\\theta^{(n)}),\n",
    "$$\n",
    "\n",
    "$$\n",
    "   \\nabla \\mathcal{R}^{\\text{emp}}(\\theta^{(n)}) = \n",
    "\\bigg(\\frac{\\partial  R^{emp}}{\\partial  w_1}(\\theta^{(n)}), ...,\\frac{\\partial  R^{emp}}{\\partial w_D}(\\theta^{(n)}) \\bigg)',\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "gradient descent update\n",
    "\"\"\"\n",
    "@jax.jit\n",
    "def update(params, u, y_true, alpha, step_size=1e-2):\n",
    "    grads = jax.grad(loss)(params, u, y_true, alpha)\n",
    "\n",
    "    return [\n",
    "        (w - step_size * dw)\n",
    "        for w, dw in zip(params, grads)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "    \\mathcal{A}(S) = \\theta_{\\text{opt}} = \\underset{\\theta \\in \\Theta}{\\text{argmin}} \\; \\underbrace{\\frac{1}{N} \\sum^N_{i=1} L(\\hat{\\mathbf{Y}}_{\\theta}^{\\text{train}}, \\mathbf{Y}^{\\text{train}})}_{\\mathcal{R}^{\\text{emp}}(\\theta)},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "s is a list of tuples \n",
    "[(u1, y1), (u2, y2), ...] -> ((u_1, u_2, ...), (y_1, y_2, ...))\n",
    "a signal u_i or y_i is (K, n_i) where n_i is the length of the song which can differ\n",
    "and K is the number of modeled drum parts\n",
    "\"\"\"\n",
    "def unpack(s):\n",
    "    f1 = map(lambda x: x[0], s)\n",
    "    f2 = map(lambda x: x[1], s)\n",
    "\n",
    "    u_batch = np.array(list(f1))\n",
    "    y_batch = np.array(list(f2))\n",
    "\n",
    "    return (u_batch, y_batch)\n",
    "\n",
    "\"\"\"\n",
    "trains the network on the training data for n epochs\n",
    "in the cross validation setup the testing data is\n",
    "the validation set and is simply used to measure the\n",
    "testing loss next to the training loss\n",
    "\"\"\"\n",
    "def train(params, u_train, y_train, u_test, y_test, alpha, n_epochs=15, batch_size=128):\n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "    for epoch in range(n_epochs):\n",
    "        start_time = time.time()\n",
    "        n_batches = math.ceil(u_train.shape[0] / batch_size)\n",
    "        for batch_idx in range(n_batches):\n",
    "            start = batch_idx * batch_size\n",
    "            end = -1 if batch_idx == (n_batches - 1) else start + batch_size\n",
    "            u_batch = u_train[start:end, :]\n",
    "            y_batch = y_train[start:end, :]\n",
    "            params = update(params, u_batch, y_batch, alpha)\n",
    "\n",
    "        train_loss.append(loss(params, u_train, y_train, alpha))\n",
    "        test_loss.append(loss(params, u_test, y_test, alpha))\n",
    "\n",
    "        epoch_time = time.time() - start_time\n",
    "\n",
    "        print(f'Epoch {epoch+1:>2} ({epoch_time:<.2f}s): ', end='')\n",
    "        print(f'train loss {train_loss[-1]:<5.2f} test loss {test_loss[-1]:<5.2f}', end='| ')\n",
    "    \n",
    "    return params, train_loss, test_loss\n",
    "\n",
    "\"\"\"\n",
    "same as train but without testing set to measure testing loss\n",
    "\"\"\"\n",
    "def train2(params, u_train, y_train, r, n_epochs=2, batch_size=128):\n",
    "    train_loss = []\n",
    "    for epoch in range(n_epochs):\n",
    "        start_time = time.time()\n",
    "        n_batches = math.ceil(u_train.shape[0] / batch_size)\n",
    "        for batch_idx in range(n_batches):\n",
    "            start = batch_idx * batch_size\n",
    "            end = -1 if batch_idx == (n_batches - 1) else start + batch_size\n",
    "            u_batch = u_train[start:end, :]\n",
    "            y_batch = y_train[start:end, :]\n",
    "            params = update(params, u_batch, y_batch, r)\n",
    "\n",
    "        train_loss.append(loss(params, u_train, y_train, r))\n",
    "\n",
    "        epoch_time = time.time() - start_time\n",
    "\n",
    "        print(f'Epoch {epoch+1:>2} ({epoch_time:<.2f}s): ', end='')\n",
    "        print(f'train loss {train_loss[-1]:<5.2f} |')\n",
    "    return params\n",
    "\n",
    "if (TRAINING_ON and not RC_ON):\n",
    "    # given S, params, loss/emprical risk, r (regularization)\n",
    "    # Hyperparameters\n",
    "    print(len(S))\n",
    "    step_size=1e-2\n",
    "    k = 10   # k fold cross validation\n",
    "    # r is the hyperparameter (here r = alpha for the regularization)\n",
    "    validation_risk_r = []\n",
    "\n",
    "    \"\"\"\n",
    "    Edge case: if k > amount of tuples (u_train, y_train) in S then n = 0\n",
    "    which will result in a valueError when constructing S_k.\n",
    "    Furthermore: the procedure will break down for single (u_train, y_train)\n",
    "    because then the reduced training set will be the empty set\n",
    "    \"\"\"\n",
    "    n = (int) (len(S)/k)\n",
    "\n",
    "    # split S into k disjoint subsets\n",
    "    S_k = [S[i : i + n] for i in range(0, len(S), n)]\n",
    "\n",
    "\n",
    "    full_train_loss = []\n",
    "    full_test_loss = [] # loss on validation set\n",
    "    for r in range(0, 5):\n",
    "        validation_risk = []\n",
    "        for j in range(0, k):\n",
    "            V = S_k.pop(j)                          # validation set\n",
    "            T = [x for l in S_k for x in l]         # reduced training set\n",
    "            S_k.insert(j, V)\n",
    "            u_train, y_train = unpack(T)\n",
    "            #print(u_train.shape)\n",
    "            #print(y_train.shape)\n",
    "            u_val, y_val = unpack(V)\n",
    "            #print(u_val.shape)\n",
    "            #print(y_val.shape)\n",
    "            params, train_loss, test_loss = train(params, u_train, y_train, u_val, y_val, r)\n",
    "            # record loss / emp.risk\n",
    "            full_train_loss = full_train_loss + train_loss\n",
    "            full_test_loss = full_test_loss + test_loss\n",
    "\n",
    "            validation_risk.append(loss(params, u_val, y_val, r))\n",
    "\n",
    "        validation_risk_r.append(np.mean(np.array(validation_risk)))\n",
    "\n",
    "    r_opt = np.argmin(np.array(validation_risk_r))\n",
    "    u_train, y_train = unpack([x for l in S_k for x in l])  # train on whole training set\n",
    "    params = train2(params, u_train, y_train, r_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (READ_PARAMETERS):\n",
    "    params = tuple(read_list(\"params\"))\n",
    "elif (SAVE_PARAMETERS):\n",
    "    write_list(\"params\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training results plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (TRAINING_ON):  \n",
    "    fig, axs = plt.subplots()\n",
    "    fig.suptitle(\"Performance of RNN on one-time step prediction of drum tracks (using JAX)\")\n",
    "    axs.set_title('Loss')\n",
    "    axs.plot(full_train_loss, label='train loss')\n",
    "    axs.plot(full_test_loss, label='test loss')\n",
    "    axs.set_ylabel('Quadratic loss')\n",
    "    axs.set_xlabel('Training epochs')\n",
    "    axs.set_ylim(0, 1.6e9)\n",
    "    axs.legend()\n",
    "\n",
    "    plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (TRAINING_ON):\n",
    "    fig, axs = plt.subplots()\n",
    "    axs.set_title('validation risk')\n",
    "    axs.plot(validation_risk_r, label='validation risk')\n",
    "    axs.set_ylabel('validation risk')\n",
    "    axs.set_xlabel('alpha parameter')\n",
    "    axs.legend()\n",
    "\n",
    "    plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3181\n"
     ]
    }
   ],
   "source": [
    "print(len(S[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Music Generation\n",
    "Get the file to prime the network with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_fun_scan(params, x, un):\n",
    "    \"\"\" Perform single step update of the network.\n",
    "    x:  (L, ) at time step n -> x: (L, ) at time step n+1\n",
    "    un: (K, )\n",
    "    \"\"\"\n",
    "    Win, W, Wout, b = params\n",
    "    x = np.tanh(\n",
    "        np.dot(Win, un) + np.dot(W, x) + b\n",
    "    )\n",
    "    y = jax.nn.sigmoid(np.dot(Wout, x))\n",
    "    return x, y\n",
    "\n",
    "y_signal = []\n",
    "\n",
    "#song = D(drumTracks_df[0], h,  151960)[0]\n",
    "selection = [S[0][0], S[1][0], S[2][0]]\n",
    "for u_prime in selection:\n",
    "    n_stop = len(u_prime)\n",
    "    \n",
    "    x = np.zeros((L,))\n",
    "    for n in range(n_stop):\n",
    "        x, y = apply_fun_scan(params, x, u_prime[n])\n",
    "        y_signal.append(y)\n",
    "\n",
    "n_output = 20000\n",
    "for n in range(n_output):\n",
    "    x, y = apply_fun_scan(params, x, y)\n",
    "    y_signal.append(y)\n",
    "\n",
    "dfo = D_inv(np.array(y_signal), h_inv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathcal{M}^{-1} : \\text{CSV} \\rightarrow \\text{MIDI}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Puts generated CSV file in correct folder\n",
    "and generates the MIDI file and puts that in the correct\n",
    "folder as well.\n",
    "\"\"\"\n",
    "\n",
    "os.chdir('../midi/NetworkOutputCSV')\n",
    "name = \"rnn.csv\"\n",
    "dfo.to_csv(name, index = False, header = False)\n",
    "copy(name, '../midicsv-1.1')\n",
    "os.chdir('../midicsv-1.1')\n",
    "\n",
    "command = \"csvmidi\" + \" \" + name + \" \" + name[:-4] + \".mid\"\n",
    "res = os.system(command)\n",
    "print(res)\n",
    "os.remove(name)\n",
    "try:\n",
    "    move(name[:-4] + \".mid\", '../NetworkOutputMIDI')\n",
    "except:\n",
    "    print(\"midi file already here\")\n",
    "    os.remove(name[:-4] + \".mid\")\n",
    "\n",
    "os.chdir(cwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental: reservoir computing\n",
    "dimensionality $K, L, K$ same: using same rnn.\n",
    "First: scaling with spectral radius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# derived from https://github.com/rug-minds/teaching/blob/main/boilerplate/reservoir.ipynb\n",
    "if (RC_ON):\n",
    "    W, Win, Wout, b = params\n",
    "    rhoW = np.max(np.absolute(np.linalg.eig(W)[0]))\n",
    "    scale = 1.25\n",
    "    W = (scale/rhoW)*W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: state harvesting: record activation for each of the $L$ reservoir neurons\n",
    "when driven by teacher input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# derived from https://github.com/rug-minds/teaching/blob/main/boilerplate/reservoir.ipynb\n",
    "def forward(u, Win, W, b, Wout=None, x_init=np.zeros((L, ))):\n",
    "    # u: (n_max, K)\n",
    "    n_max = u.shape[0]\n",
    "    K = u.shape[1]\n",
    "    X, Y = [], []\n",
    "    # X: (1+K+L, n_max)\n",
    "    # Y: (K, n_max) double chheck later\n",
    "    x = x_init.copy()\n",
    "    for t in range(u.shape[0]):\n",
    "        x = np.tanh(\n",
    "            np.dot(Win, u[t]) + np.dot(W, x) + b\n",
    "        )\n",
    "        full_state = np.concatenate(u[t], x, b)\n",
    "        X.append(full_state)\n",
    "        if Wout is not None:\n",
    "            y = np.dot(Wout, full_state)\n",
    "            Y.append(y)\n",
    "        # when returning: need to transpose data arrays, such that dim: (n_max, x)\n",
    "    if Wout is None:\n",
    "        return x, np.array(X).T\n",
    "    else:\n",
    "        return x, np.array(X).T, np.array(Y).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# derived from https://github.com/rug-minds/teaching/blob/main/boilerplate/reservoir.ipynb\n",
    "if (RC_ON):\n",
    "    x, X = forward(u_train[0], Win, W)\n",
    "    print(f'u: {u_train[0].shape}, x: {x.shape}, X: {X.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 2: compute readouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# derived from https://github.com/rug-minds/teaching/blob/main/boilerplate/reservoir.ipynb\n",
    "if (RC_ON):\n",
    "    reg = 1e-8\n",
    "\n",
    "    Wout_rc_ = np.dot(\n",
    "        np.dot(y_train[0].T, X.T),\n",
    "        np.linalg.inv(\n",
    "            np.dot(X, X.T) + reg*np.eye(1+K+L)\n",
    "        )\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e375b77a672b76be28c1e80386a6d4db61c866c142c6acff7bc94a65b4573147"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
